\section*{Algorithm} 
\addcontentsline{toc}{section}{\protect\numberline{}Algorithm description}

By defining Neural Networks as (\ref{nn_functional_space}).  Fixed the activation function 
$\gamma$, we can see a Neural Network $h$ of $n$ hidden 
units, $d \in \N$ entry dimension and $s$ output
dimension as 
\begin{align}
    &h: \R^d \longrightarrow \R^s,\\
    &h_k(x) = \sum_{i= 1}^n 
        \left(
            \beta_{i k} 
            \gamma
            \left( 
                \sum_{j = 1}^d
                (
                    \alpha_{i j} x_j
                ) + z_i
            \right)
        \right).
\end{align}
determined by its params: 
\begin{align}
    (A,B,S) \in R^{n \times s} \times R^{d \times n} \times R^{d}.
\end{align} 
\begin{align*}\label{eq:representation red neuronal}
    A &= (\alpha_{i j}) \text{ con }  i \in \{1, \ldots d\}, \; j \in \{1, \ldots n\}. \\
    S &= (\alpha_{0 j}) \text{ con }  j \in \{1, \ldots, n\}. \\
    B &= (\beta_{j k}) \text{ con }  j \in \{1, \ldots n\}, \; k \in \{1, \ldots s\}.
\end{align*}

For our algorithm we will determine the value of 
$(A,B,S)$ seeing them and the training data as an oversized system of linear equations. 

\subsection*{Algorithm description}  

Let be $h \in  \mathcal{H}(\R^d, \R^s)$
 with $n$ hidden units. 
and let $M \in R^+$ chosen
 conveniently\footnote{See subsection \textit{Some values for $M$}\ref{subsec:values of M}}. 

 \begin{enumerate}
    % First steps, initialize de values
    \item Take randomly $p \in \R^{d+1} \setminus \{0\}$.
    \item Let $\Lambda$ be an empty set. 
    \item While $|\Lambda| < n$ repeat:
    \begin{enumerate}[label=\roman*.]
        \item Pick randomly $(x,y) \in \mathcal{D}$.
        \item If $x$ satisfies that for every $(z,w) \in \Lambda$
        \begin{equation}
            p \cdot (x - z) \neq 0,
        \end{equation}
        then let $\Lambda \gets \Lambda \cup \{(x,y)\}$.
    \end{enumerate}
    % Ordered sed 
    \item Without loss of generality the elements of $\Lambda$
    \begin{equation*}
        \Lambda = \{(x_1,y_1), (x_2,y_2), \ldots (x_n, y_n)\}
    \end{equation*}
    are ordered by the following statement
    \begin{equation}
        p \cdot x_1 < p \cdot x_2< \ldots p \cdot x_n.
    \end{equation}
    % Setting the parameters 
    \item Pick  $(x_1, y_1) \in \Lambda$ \\
    \begin{align*}
         &S_1 = M p_0, \\
         & A_{1 *} = M p_{[1,d]}, \\
         & B_{* 1} = y_1.
     \end{align*}
    For $k \in \{1, \ldots, n \}$
     \begin{align*}
         &S_{k} = M -  \frac{2 M}{p \cdot (x_k - x_{k-1})}(p \cdot x_{k}),\\
         & A_{k i} = \frac{2 M}{p \cdot (x_k - x_{k-1})}
         p_{i}  \quad i \in \{1, \ldots d\},\\
         & B_{* k} = y_k - y_{k-1}.
     \end{align*} 
     where $(x_k, y_k) \in \Lambda$.
     
     \item $(A,S,B)$ are the matrix we searched for.
    
 \end{enumerate}
 $p \in \R^d$ and $\Lambda \subset \mathcal{D}$ subset of the training set, the system for our Neuronal Model is defining as 
\begin{equation}\label{eq: neural system}
    \left\{ 
        \begin{array}{l}
            S_{k} =
                M -  \frac{2 M}{p \cdot (x_k - x_{k-1})}(p \cdot x_{k})
            \\
            A_{k i} = \frac{2 M}{p \cdot (x_k - x_{k-1})}
            p_{i} 
            \\
            B_{* k} = y_k - y_{k-1}
        \end{array}
    \right.
\end{equation}  
where $(x_k, y_k) \in \mathcal{D}$ and $S_{k}$ denoted the $k-$th column, $A_{k i}$ the element of the $k$ row and $i$ column, $B_{* k}$ denoted the $k-$th row. 

\subsubsection*{Some values for $M$}
\label{subsec:values of M}
The value of $M$ is determined by the chosen activation function, $M$ could be any real value bigger than the specified one: 

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
    \hline
        Activation function  & Minimum value of $M$ \\ \hline
        Ramp function & 1 \\ \hline
        \textit{Cosine Squasher} & $\frac{\pi}{2}$ \\ \hline
        Indicator function of 0 & 0 \\ \hline
        \textit{Hardtanh} & 1 \\ \hline
        Sigmoidea  &  $10$ (for an error smaller than $10^{-5}$)\\ \hline
        Tangente hiperbÃ³lica  &  $7$ for an error smaller than $10^{-5}$)\\ \hline
    \end{tabular}
    \caption{
        Minimum value of $M$ for the algorithm depending of the chosen activation function.
    }
    \label{table:M-activation-function}
\end{table}




