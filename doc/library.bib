%----- Teoría de la aproximación -----
% Texto principal del que se ha sacado 
@book{the-elements-of-real-analysis,
  title={The elements of real analysis},
  author={Robert G. Bartle},
  year={1947},
  publisher={John. Wiley \& Sons}
}

% ---------- Introducción a las redes neuronales -------

@book{learning-from-data-1-2,
  title={Learning From Data: Concepts, Theory, and Methods},
  author={Vladimir Cherkassky and Filip Mulier},
  year={2007},
  publisher={John. Wiley \& Sons},
  edition        = {2}, 
  chapter        = {1,2},
}

@online{e-chapter-7-neural-networks,
  author = { Yaser Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Li},
  title = {{Neural Networks}},
  year = {2015},
  urldate = {Jan-2015},
  chapter = {7},
}

% algoritmo backpropagation 
@article{backpropagation-Hinton,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	da = {1986/10/01},
	date-added = {2022-02-22 07:39:52 +0100},
	date-modified = {2022-02-22 07:40:25 +0100},
	doi = {10.1038/323533a0},
	id = {Rumelhart1986},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533--536},
	read = {0},
	title = {Learning representations by back-propagating errors},
	ty = {JOUR},
	url = {https://doi.org/10.1038/323533a0},
	volume = {323},
	year = {1986},
	Bdsk-Url-1 = {https://doi.org/10.1038/323533a0}
  }


% historia redes neuronales  
@online{hisour,
  author = {Hisour},
  title = {Nocción del aprendizaje automático},
  year = 2021,
  url = {https://www.hisour.com/es/machine-learning-42773/},
  urldate = {2021-11-27}
}

@online{samuel-wikipedia,
  author= {Wikipedia. Arthur Samuel},
  url = {https://en.wikipedia.org/wiki/Arthur_Samuel},
  urldate = {2021-11-30}
}
@book{tom-michell-machine-learning,
   title={Machine Learing},
   author={Tom Mitchell},
   year={1997},
   publisher={McGraw},
    url = {https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html},
}
@online{mitchell-wikipedia,
  author= {Wikipedia. Tom Mitchell},
  url = {https://en.wikipedia.org/wiki/Tom_M._Mitchell},
  urldate = {2021-11-30}
}


%%% ----- Construcción de las redes neuronales   --------
%% Manual muy extenso , lo has usado para
%% Definición perceptrón 
%% Feedforward networks functions: capítulo 5 páginas 227-256
@book{BishopPaterRecognition,
author = {Bishop, Christopher M.}, 
title = {Pattern Recognition and Machine Learning (Information Science and Statistics)}, 
year = {2006}, 
isbn = {0387310738}, 
publisher = {Springer-Verlag}, 
address = {Berlin, Heidelberg} 
}

%% Manual también muy interesante
%% Usado para los tipos de aprendizaje Capítulo 1
@book{MostafaLearningFromData,  
author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien}, 
title = {Learning From Data}, 
year = {2012}, 
isbn = {1600490069},
 publisher = {AMLBook}, 
 abstract = {Machine learning allows computational systems to adaptively 
 improve their performance with experience accumulated from the observed
data. Its techniques are widely applied in engineering, science, finance, and commerce.
This book is designed for a short course on machine learning. It is a short course, not 
a hurried course. From over a decade of teaching this material, we have distilled what
we believe to be the core topics that every student of the subject should know. 
We chose the title `learning from data' that faithfully describes what the subject 
is about, and made it a point to cover the topics in a story-like fashion. 
Our hope is that the reader can learn all the fundamentals of the subject by 
reading the book cover to cover. ---- Learning from data has distinct theoretical
and practical tracks. In this book, we balance the theoretical and the practical,
the mathematical and the heuristic. Our criterion for inclusion is relevance. 
Theory that establishes the conceptual framework for learning is included, 
and so are heuristics that impact the performance of real learning systems. 
---- Learning from data is a very dynamic field. Some of the hot techniques
and theories at times become just fads, and others gain traction and become
part of the field. What we have emphasized in this book are the necessary 
fundamentals that give any student of learning from data a solid foundation,
and enable him or her to venture out and explore further techniques and theories, 
or perhaps to contribute their own. ---- The authors are professors at California 
Institute of Technology (Caltech), Rensselaer Polytechnic Institute (RPI), 
and National Taiwan University (NTU), where this book is the main text for 
their popular courses on machine learning. The authors also consult extensively 
with financial and commercial companies on machine learning applications,
and have led winning teams in machine learning competitions.}          
}


%% Ejemplos de tipo de aprendizaje en los que se utiliza redes neuronales: 

% artículo donde en el estado del arte dice que se aplica en 
% todas los tipos de aprendizajes 
%y indica como mejorarlo en el no supervisado 

@INPROCEEDINGS{8612259,
  author={Dike, Happiness Ugochi and Zhou, Yimin and Deveerasetty, Kranthi Kumar and Wu, Qingtian},
  booktitle={2018 IEEE International Conference on Cyborg and Bionic Systems (CBS)}, 
  title={Unsupervised Learning Based On Artificial Neural Network: A Review}, 
  year={2018},
  volume={},
  number={},
  pages={322-327},
  doi={10.1109/CBS.2018.8612259}}

% para aprendizaje por refuerzo 
@article{DBLP:journals/corr/BakerGNR16,
  author    = {Bowen Baker and
               Otkrist Gupta and
               Nikhil Naik and
               Ramesh Raskar},
  title     = {Designing Neural Network Architectures using Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1611.02167},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.02167},
  eprinttype = {arXiv},
  eprint    = {1611.02167},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BakerGNR16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% Ejemplo que utiliza algoritmos evolutivos para configurar la
red neuronal 
@inproceedings{10.5555/2955491.2955578, 
author = {Stanley, Kenneth O. and Miikkulainen, Risto}, 
title = {Efficient Reinforcement Learning through Evolving Neural Network Topologies}, 
year = {2002}, 
isbn = {1558608788}, 
publisher = {Morgan Kaufmann Publishers Inc.}, 
address = {San Francisco, CA, USA}, 
abstract = {Neuroevolution is currently the strongest method on the pole-balancing benchmark reinforcement learning tasks. 
Although earlier studies suggested that there was an advantage in evolving the network topology as well as connection weights, the leading neuroevolution systems evolve fixed networks. 
Whether evolving structure can improve performance is an open question. In this article, we introduce such a system, NeuroEvolution of Augmenting Topologies (NEAT). We show that 
when structure is evolved (1) with a principled method of crossover, (2) by protecting structural innovation, and (3) 
through incremental growth from minimal structure, learning is
 significantly faster and stronger than with the best 
 fixed-topology methods. NEAT also shows that it is possible to evolve populations of increasingly large genomes, achieving 
 highly complex solutions that would otherwise be difficult to optimize.}, 
 booktitle = {Proceedings of the 4th Annual 
 Conference on Genetic and Evolutionary Computation}, 
 pages = {569–577}, 
 numpages = {9}, 
 location = {New York City, New York}, 
 series = {GECCO'02} 
}

%% Sobre el perceptrón 

@InProceedings{perceptron-convergence,
author="Van Der Malsburg, C.",
editor="Palm, G{\"u}nther
and Aertsen, Ad",
title="Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
booktitle="Brain Theory",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="109-115",
abstract="Frank Rosenblatt's intention with his book, according to his own introduction, is not just to describe a machine, the perceptron, but rather to put forward a theory. He formulates a series of machines. Each machine serves to introduce a new concept.",
isbn="978-3-642-70911-1"
}
%% ------ Redes neuronales como aproximadores universales  -----
% Conferencia de 1987 sobre redes neuronales
@ARTICLE{4307059,
  author={},
  journal={IEEE Expert}, 
  title={IEEE First Annual International Conference on Neural Networks San Diego, California June 21-24, 1987}, 
  year={1987},
  volume={2},
  number={2},
  pages={14-14},
  doi={10.1109/MEX.1987.4307059}}

% Artículo principal: Multilayer feedforward networks are universal approximators
@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}
% Imagen perceptrón multicapa  
@misc{alma991008058419704990,
publisher = {AMLbook.com},
title = {Learning from data : a short course },
year = {2012},
author = {Abu-Mostafa, Yaser S.},
address = {Seattle},
booktitle = {Learning from data : a short course},
isbn = {9781600490064},
keywords = {Aprendizaje automático},
language = {eng},
}
% Libro para algunas demostraciones de teoría de la medida
% Se cita en pag 228 teorema 52.G
% Se cita en pag 241-242  teorema 55.C y 55.D
% Para el teorema de Lusin en pag 242-243.
@Book{ nla.cat-vn1819421,
author = { Halmos, Paul R. },
title = { Measure theory / [by] Paul R. Halmos },
isbn = { 0387900888 },
publisher = { Springer-Verlag New York },
year = { 1974 },
type = { Book },
url = { http://www.loc.gov/catdir/enhancements/fy0814/74010690-t.html },
language = { English },
subjects = { Measure theory. },
life-dates = { 1974 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn1819421 },
}

% Ejemplos de redes neuronales que combinan distintas funciones de activación 
% artículo que compara arquitecturas habituales 
@article{DBLP:journals/corr/abs-1811-03378,
  author    = {Chigozie Nwankpa and
               Winifred Ijomah and
               Anthony Gachagan and
               Stephen Marshall},
  title     = {Activation Functions: Comparison of trends in Practice and Research
               for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1811.03378},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.03378},
  eprinttype = {arXiv},
  eprint    = {1811.03378},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-03378.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% Ejemplo arquitectura 
@article{DBLP:journals/corr/SzegedyVISW15,
  author    = {Christian Szegedy and
               Vincent Vanhoucke and
               Sergey Ioffe and
               Jonathon Shlens and
               Zbigniew Wojna},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  journal   = {CoRR},
  volume    = {abs/1512.00567},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.00567},
  eprinttype = {arXiv},
  eprint    = {1512.00567},
  timestamp = {Mon, 13 Aug 2018 16:49:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% ejemplo paper general 
@INPROCEEDINGS{8258768,
  author={Wang, Bin and Li, Tianrui and Huang, Yanyong and Luo, Huaishao and Guo, Dongming and Horng, Shi-Jinn},
  booktitle={2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)}, 
  title={Diverse activation functions in deep learning}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ISKE.2017.8258768}}

%%% ---- Sobre funciones de activación 
  % Artículo donde se introduce la función de cosine squasher
  @MISC{Gallant88thereexists,
    author = {A. Ronald Gallant and Halbert White},
    title = {There Exists A Neural Network That Does Not Make Avoidable Mistakes},
    year = {1988},
    abstract = {
    We show that a multiple input, single output, 
    single hidden layer feedforward network with (known) 
    hardwired connections from input to hidden layer, monotone squashing at the hidden layer
    and no squashing at the output embeds as a special case a "Fourier network" which yields a Fourier series
    approximation to a given function as its output. Thus, such networks possess all the approximation properties
    of Fourier series representations. In particular, approximation to any desired accuracy of any square 
    integrable function can be achieved by such a network, using sufficiently many hidden units. In this 
    sense, such networks do not make avoidable mistakes.
    }
}
% Generalización con funciones no continuas
@article{FUNAHASHI1989183,
title = {On the approximate realization of continuous mappings by neural networks},
journal = {Neural Networks},
volume = {2},
number = {3},
pages = {183-192},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90003-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900038},
author = {Ken-Ichi Funahashi},
keywords = {Neural network, Back propagation, Output function, Sigmoid function, Hidden layer, Unit, Realization, Continuous mapping},
abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.}
}
% Generalización del teorema universal con funciones de actvación no acotadas
@article{DBLP:journals/corr/SonodaM15,
  author    = {Sho Sonoda and
               Noboru Murata},
  title     = {Neural Network with Unbounded Activations is Universal Approximator},
  journal   = {CoRR},
  volume    = {abs/1505.03654},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.03654},
  eprinttype = {arXiv},
  eprint    = {1505.03654},
  timestamp = {Mon, 13 Aug 2018 16:47:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SonodaM15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

  %% ---------- Nociones topología -------
 % De james1966topology tomamos: 
 % Definición vecino (pag 63)
 % Definición Hausdorff (pag 137-138)
 % Definción espacio Normal  Hausdorff (pag 144)
 % Caracterización de normalidad de Tietze (pag 149-151)
  @book{james1966topology,
  title={Topology},
  author={James Dugundji},
  isbn={9780205002719},
  lccn={66010940},
  series={Allyn and Bacon series in advanced mathematics},
  url={https://books.google.es/books?id=FgFRAAAAMAAJ},
  year={1966},
  publisher={Allyn and Bacon}
}

%%%%%%%%%%%%%%%%%% metodología %%%%%%%%%%%%%%
%% Descripción del desarrollo ágil en la ciencia
@article{DBLP:journals/corr/abs-2104-12545,
  author    = {Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title     = {Agile (data) science: a (draft) manifesto},
  journal   = {CoRR},
  volume    = {abs/2104.12545},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.12545},
  eprinttype = {arXiv},
  eprint    = {2104.12545},
  timestamp = {Mon, 03 May 2021 17:38:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-12545.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% sobre cómo hacer un TFG
@online{que-es-un-trabajo-fin-de-x,
  author= {Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Cómo llevar a término un TFG/TFM en informática},
  url = {https://jj.github.io/que-es-un-trabajo-fin-de-x/tf.html},
  urldate = {2022-02-05}
}

% sobre metodología de personas 
% https://www.interaction-design.org/literature/article/personas-why-and-how-you-should-use-them
@online{personas-why-and-how-you-should-use-them,
  author= {Rikke Friis Dam and Teo Yu Siang |},
  title ={Personas – A Simple Introduction},
  url = {https://www.interaction-design.org/literature/article/personas-why-and-how-you-should-use-them},
  urldate = {2022-02-05}
}

% mi repositorio de github :D
% https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales
@online{TFG-Estudio-de-las-redes-neuronales,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s, Francisco Javier Mer{\'{i}} de la Maza },
  title ={Github, repositorio: Estudio de las redes neuronales},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales},
  urldate = {2022-02-05}
}
% HISTORIAS DE USUARIO 
% HU 01
@online{TFG-Estudio-de-las-redes-neuronales-HU01,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 1},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/48},
  urldate = {2022-02-13}
}
% HU 06
@online{TFG-Estudio-de-las-redes-neuronales-HU02,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 2},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/65},
  urldate = {2022-02-13}
}
% HU 03
@online{TFG-Estudio-de-las-redes-neuronales-HU03,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 3},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/50},
  urldate = {2022-02-13}
}
% HU 04
@online{TFG-Estudio-de-las-redes-neuronales-HU04,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 4},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/51},
  urldate = {2022-02-13}
}
% HU 05
@online{TFG-Estudio-de-las-redes-neuronales-HU05,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 5},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/64},
  urldate = {2022-02-13}
}
% HU 06
@online{TFG-Estudio-de-las-redes-neuronales-HU06,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 6},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/49},
  urldate = {2022-02-13}
}

%% Milestone
@online{TFG-Estudio-de-las-redes-neuronales-milestones,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s, Francisco Javier Mer{\'{i}} de la Maza},
  title ={Github, repositorio: Estudio de las redes neuronales, Milestone},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/milestones},
  urldate = {2022-02-13}
}

%%%%%% registro del número de horas de trabajo %%%%%%%%%%%%%
%% hoja de cálculo 
%% https://docs.google.com/spreadsheets/d/1TCcKQIKjKW9sMSU2f6obN9gHgv3c8UEdjmONkBlv42M/edit?usp=sharing
@online{TFG-hoja-calculo-horas-trabajo,
  author= {Blanca Cano Camarero},
  title ={Registro de trabajo en hoja de cálculo},
  url = {https://docs.google.com/spreadsheets/d/1TCcKQIKjKW9sMSU2f6obN9gHgv3c8UEdjmONkBlv42M/edit?usp=sharing},
  urldate = {2022-02-05}
}
