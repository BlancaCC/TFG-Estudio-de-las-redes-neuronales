% !TeX root = ../../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Perceptrón
%*******************************************************

\chapter{Construcción de las redes neuronales}  
Para entender con mayor claridad cómo se construye una red neuronal, explicaremos primero 
uno a uno cada uno de sus elementos clave 
(Capítulo 4 \cite{BishopPaterRecognition}).

\section{El perceptrón}  

El perceptrón de Rosenblatt fue presentado en 1962. 
Y no es más que un ejemplo de modelo lineal 
discriminate a dos clases.
Dado un vector de entrada $x$, 
$\phi$ una transformación no lineal fija que relaciona
$x$ con un vector de características. 

Por lo general, $\phi(x)$ suele incluir un sesgo, 
devolviendo un vector, con un uno en la primera componente y en las siguientes las misma que $x$. 
\begin{equation}
    \phi((x_1,..x_n)) = (1, x_1, ..., x_n)
\end{equation} 

La fórmula generalizada es 
\begin{equation}
    y(x) = f(w^T \phi(x))
\end{equation}

Donde $w$ es un vector de pesos a ajustar y 
$f$ una función a trozos definida como 

\begin{equation}
    f(x) =
    \left\{ 
        \begin{aligned}
        +1, & x \geq 0
            \\
            -1, & x \leq 0.
        \end{aligned}
    \right .  
\end{equation}

\subsubsection{El algoritmo del perceptrón}

Se define como función de pérdida la conocida como 
\textit{criterio del perceptrón}

\begin{equation}
    E_p(w) = - \sum_{x \in \mathcal M} w^T \phi_n (x_n)
\end{equation}

Donde $\mathcal M$ denota al conjunto de puntos mal clasificados por el perceptrón.

Aplica el algoritmo del gradiente descendente estocástico, la sucesión de pesos será de la forma

\begin{equation}
    w_{\tau+1} = 
    w_{\tau} - \eta \nabla E_p(w)
     = 
     w_{\tau} + \eta \phi_n (x_n)
\end{equation}

donde $\eta$, el denominado  \textit{learning rate}
es un coeficiente libre real, que determina la relevancia del gradiente del error en la actualización. 

\subsubsection{Gradiente descendente}  

El gradiente descendente es una técnica usada para minimizar funciones diferenciables
a partir de la pendiente en cada punto. 

Se define punto inicial y arbitrario $w_0$ y una función de error diferenciable
$E: \R^d \times \R^d \longrightarrow \R$, así como un \textit{learning rate} 
$\eta \in \R^+$. 
El método de actualización es 
\begin{equation}
    w_{t+1}  = w_t - \eta \nabla E(w_t).
\end{equation}

Las características de este algoritmo son las siguientes: 

\begin{itemize}
    \item El algoritmo solo encuentra óptimos locales con una dependencia crucial del valor de inicio. 
    \item La convergencia no es segura en un tiempo finito y requiere de criterios de parada. 
    \item Si la función es convexa el mínimo será global. 
\end{itemize}

\subsubsection{Convergencia del algoritmo del perceptrón}  

El teorema de convergencia del perceptrón establece que si existe una solución, esto es
que el conjunto de los datos sea separables, entonces el algoritmo del perceptrón
nos asegura una convergencia exacta en un número finito de pasos. 
Una demostración fue dada por Rosenblatt en 1962 \cite{perceptron-convergence}.  

Características generales que posee este algoritmo es que
cada paso del algoritmo no asegura la disminución del error y en la práctica, hasta que no se 
ha llegado a la convergencia se desconoce si los datos son separables o no. 











