% !TeX root = ../../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Introducción artículo MFNAUA
%*******************************************************


\section{Definiciones esenciales}  

Se pretende con las siguientes definiciones una concepción y trabajo preciso de la clase de redes neuronales multicapa feedforward. 


\begin{definicion} Función de activación \\
    Una función  $\psi: \R \longrightarrow [0,1]$ es una \textbf{ función de activación} si  cumple las siguientes propiedad
    \begin{enumerate}[label=(\roman*)]
        \item Es no decreciente
        \item $\lim _{x \rightarrow \infty} \psi(x) = 1
        $.
        \item $\lim _{x \rightarrow -\infty} \psi(x) = 0$.
    \end{enumerate}  

    Las funciones de activación son medibles ya que tienen como mucho un número contable de discontinuidades.
   
    Ejemplos comunes son
    \begin{itemize}
        \item Funciones umbral. 
        (Vale cero hasta que se cumple cierta condición de las entradas, a partir de ahí empezaría a valer uno).

        \item Funciones indicadoras: $\psi(\lambda) = 1_{\{\lambda > 0\}}$. 
        \item Ramp function: $\psi(\lambda)  = \lambda 1_{\{0 \leq \lambda \leq  1\}} + 1_{\{\lambda > 1\}}$
    
        \item La función \textit{cosine squasher} de Gallant and White (1988)
        \begin{equation*}
    \psi(lambda )= (1 + cos(\lambda + 3 \frac{\pi}{2}) \frac{1}{2}) 
     1_{\{\frac{-\pi}{2} \leq \lambda \leq  \frac{\pi}{2}\}}
     1_{\{ \frac{\pi}{2} < \lambda \}}
    \end{equation*}
    \end{itemize}

    Cabe destacar que la definición tomada es la propuesta en \cite{HORNIK1989359} y que en otros casos 
    hay quien acepta funciones otras posibles definiciones menos restrictivas.
\end{definicion}

\begin{definicion} Función afín

    Para cualquier natural mayor que cero $r$  denotaremos por $A^r$ como al conjunto de todas 
    las funciones afines de $\R^r$ a $\R$. Es decir el conjunto de funciones de la forma 
    $A(x) = w \cdot x + b$ donde $x$ y $w$ son vectores de $\R^r$ y $\cdot$ representa el producto 
    usual de escalares y $b \in \R$ es un escalar.  
    
\end{definicion}  

En este contexto, $x$ corresponde al vector entrada de la red neuronal, $w$ los pesos de la red
que se multiplicarán con $x$ en la capa intermedia y $b$ el sesgo. 

Nótese que faltaría componer la función afín con una función de activación para obtener lo que hemos definido 
como un perceptrón. 

\begin{definicion} Primera formalización red neuronal multicapa \\
    Para cualquier función Borel medible $G(\cdot)$, definida de $\R$ a $\R$ y cualquier natural positivo ç
    $r \in \N$ de define a la clase de funciones $\pmc$ como 
    \begin{equation}
        \begin{split}
        \pmc = 
        \{ 
            & f: \R ^r \longrightarrow \R \\
            &f(x)=\sum_{j = 1} ^q (
            \beta_j G(A_{j}(x)), \\
            & x  \in \R ^r, \beta_j \in \R, A_{j}\in A^r, l_j,q \in \N
            )
        \}
        \end{split}
    \end{equation}
\end{definicion}


Obsérvese que cada sumatoria representa un perceptrón y la salida final 
es una combinación lineal de los distintos perceptrones. 

Con esta misma idea se definirá la siguiente estructura que generaliza a la familia $\pmc$  
   
\begin{definicion} Generalización de la familia $\pmc$ \\
    
    \begin{equation} 
        \begin{split}
        \sum \prod^r(G) = \{ 
        &f: \R^r \longrightarrow \R | \\
        & f(x) = \sum_{j = 1} ^q  \beta_j \cdot \prod_{k=1}^{l_j}
        G(A_{jk}(x)), \\
        &x  \in \R^r, \beta_j \in \R, A_{jk}\in A^r, l_j,q \in \N
        )
        \}
    \end{split}
    \end{equation}  

    Notemos que $\pmc$ es el caso particular $l_j = 1$ para todo $j$. Nuestro primer
    resultado se demostrará para $\sum \prod^r(G)$ y como consecuencia quedará probado
    también para $\pmc$. 
\end{definicion}


\subsection{ Reflexión sobre la relevancia de la función de activación}  

La función de activación acota la imagen de un una aplicación afín, esta limitación 
es de total relevancia ya que permite una interpretación previamente convenida. 
Decimos por ende que es la causante del \textit{aprendizaje}.     

Por si mi comentario no ha resultado claro procedo a dar un ejemplo:   
Supongamos que nos hallamos frente a un problema de clasificación de dos clases y que la función de activación
tomada es una función umbral que toma valores cero o uno. Define por tanto esta función una separación de clases. 
De otra manera, de solo existir la función afín sin una transformación de la salida, el codominio serían los Reales
donde a priori no se explicita una asignación de clase.  

Introduciremos a continuación notación de los conjuntos de funciones que seremos capaces de aproximar.  

\begin{definicion} Conjuntos de funciones continuas y Borel-medibles \\
    Sea $C^r$ el conjunto de funciones continuas con dominio en $\R^r$ y codominio $R$, 
    y sea $M^r$ el conjunto de todas las funciones Borel medible de $\R^r$ a $\R$. 

    Denotaremos también a la $\sigma$-álgebra de Bore en $\R ^r$ como $B^r$. 
\end{definicion}

En lo que respecta a definiciones anteriores, $\pmc$ y $\pmcg$ pertenecen a 
$M^r$ para cualquier función Borel-medible $G$. Si $G$ es continua entonces 
$\pmc$ y $\pmcg$ pertenecen a $C^r$. Tengamos presente que $C^r$ es un subconjunto ç
de $M^r$.  

Como veremos más adelante primero demostraremos el teorema para $C^r$ y después 
extenderemos el resultado para $M^r$.   


\subsection{ Reflexión sobre el tipo de conjuntos que se pueden aproximar}

La existencia de conjuntos de funciones no medibles manifiesta una limitación
de la formalización actual de las redes neuronales. Que plantea las siguientes 
preguntas: 
\begin{enumerate}
    \item ¿Supone la existencia de este conjunto una verdadera limitación a nivel práctico?
    \item ¿Se podría construir alguna arquitectura que sí que las aproximara?
\end{enumerate}  

Continuando con el hilo de la segunda cuestión, si se carece de un espacio vectorial, 
de una medida,  ¿Cómo se podría construir una sucesión de funciones que se aproxime?
Quizás habría que buscar características más intrínsecas del problema en cuestión, 
razonamientos topológicos.

\begin{definicion} Subconjunto denso \\  
    Dado un subconjunto $S$ de un espacio métrico $(X, \rho)$, se dice que $S$ es denso por la distancia $\rho$
    en subconjunto $T$ si para todo $\epsilon$ positivo y cualquier $t \in T$ existe un $s \in S$ tal 
    que $\rho(s,t) \leq \epsilon$. 
\end{definicion}

Un ejemplo habitual sería en el espacio métrico $(\R, |.|)$ con $|.|$ el valor absoluto, el subconjunto 
$T = \R$ y $S$ los números irracionales, $S = \R \setminus \Q$. 

A nivel intuitivo sería decir que los elementos de $S$ son capaces de aproximar cualquier elemento de $T$
con la precisión que se desee. 

\begin{definicion} 
    Un subconjunto $S$ de $C^r$ se dice que es \textbf{uniformemente denso por compactos} en  $C^r$
    si para cada subconjunto compacto $K \subset \R^r$ se tiene que $S$ es denso según $\rho_k$ en $C^r$
    donde está definida como sigue.
    Para cualquier $f,g \in C^r$ 
    \begin{equation}
        \rho _ k (f,g) = \sup_{x \in K} |f(x) - g(x)|.
    \end{equation}
\end{definicion}

\begin{definicion}
    Una serie de funciones $\{f_n\}$ \textbf{converge uniformemente a una función $f$ en compactos} si para 
    todos los conjuntos compactos $K \subset \R^r$ si 
    \begin{equation}
        \rho_k (f_n, f) \longrightarrow 0 \text{ cuando } n \longrightarrow \infty.
    \end{equation} 
\end{definicion}

%%%%% primer teorema de convergencia   
\begin{teorema} [Teorema de convergencia real en compactos]  \label{teo:TeoremaConvergenciaRealEnCompactosDefinicionesEsenciales}

    Sea G cualquier función continua no constante definida de $\R$ en $\R$. 
    Se tiene que $\pmcg$ es uniformemente denso por compactos en $C^r$
\end{teorema}

\begin{proof}
    Bastará con probar el conjunto $\pmcg$ satisface las hipótesis del teorema de Stone-Weiertrass \ref{ch:TeoremaStoneWeiertrass}
    La primera hipótesis será comprobar que $\pmcg$ es un álgebra, para ello veamos que:         
    \begin{enumerate}
        \item La función constante uno pertenece al conjunto. 
        Como $G$ no es constante existirá un valor de la imagen distinto de $0$, supongamos que $G(a)= b \neq 0$ para $a,b \in \R.$
        Consideremos la función afín $A(x) = 0 \cdot x + a$, está claro que $\frac{1}{b}G(A(x))$ es la función constantemente uno. 
        \item Cerrado para sumas y producto para escalares reales. 
        Si $f,g$ pertenece a  $\pmcg$, entonces $\gamma f + \sigma g$ pertenece a $\pmcg$ .
        
        Tenemos por definición del conjunto $\pmcg$
        que $f,g$ serán de la forma $f = \sum_{j = 1} ^q  \beta_{fj} \cdot \prod_{k=1}^{l_{fj}}  G(A_{fjk}(x))$ y 
        $g = \sum_{j = 1} ^p  \beta_{gj} \cdot \prod_{k=1}^{l_{gj}}G(A_{gjk}(x))$  por lo que
        \begin{equation}
            \begin{split}
                \gamma f+ \sigma g =& \gamma \sum_{j = 1} ^q  \beta_{fj} \cdot \prod_{k=1}^{l_{fj}}  G(A_{fjk}(x)) + 
                \sigma \sum_{j = 1} ^p  \beta_{gj} \cdot \prod_{k=1}^{l_{gj}}G(A_{gjk}(x)) \\
                & = \sum_{j = 1} ^q  (\gamma \beta_{fj}) \cdot \prod_{k=1}^{l_{fj}}  G(A_{fjk}(x)) + 
                \sum_{j = 1} ^p  (\sigma \beta_{gj}) \cdot \prod_{k=1}^{l_{gj}}G(A_{gjk}(x)).
            \end{split}
        \end{equation}
        
        Puede apreciarse como la suma de ambas seguirá la expresión de los elementos de $\pmcg$. 
        
        \item Cerrado para producto. Para $f,g \in \pmcg$, se tiene que $fg$ pertenece a $\pmcg$. 
        Renombrando los índices de la sumatoria con $\Lambda = i\{1..l_i\} \cup j\{1..l_j\}$ basta ver que 
        \begin{equation}
            \begin{split}
                fg &= \left(\sum_{i \in I_f} \beta_{j} \cdot \prod_{k=1}^{l_{i}}  G(A_{ik}(x))\right)
                    \left(\sum_{j \in I_g} ^p  \beta_{j} \cdot \prod_{k=1}^{l_{j}} G(A_{jk}(x)) \right) \\
                    & = \sum_{i \in I_f} \left(  \beta_{j} \cdot \prod_{k=1}^{l_{i}}  G(A_{ik}(x))
                        \left( \sum_{j \in I_g} ^p  \beta_{j} \cdot \prod_{k=1}^{l_{j}} G(A_{jk}(x))  \right)  
                     \right) \\
                    & =  \sum_{(i,j) \in I_f \times I_g} (\beta{i}\beta{j}) \prod_{k \in \Lambda} G(A_{k}(x))
            \end{split}
        \end{equation}
        Luego $fg \in \pmcg$. 
    \end{enumerate}

    La segunda hipótesis, dado cualquier compacto $K \subset \R^r$ veamos que $\pmcg$ separa puntos de $K$. 

    Por ser $G$ no constante existirán $a,b \in \R$ distintos cumpliendo que $G(a) \neq G(b)$. Tomaremos entonces cualquiera de las 
    funciones afines de $A^r$ que cumplen que $A(x) = a$ y $A(y)=b$ 
    \footnote{Sabemos que al menos una habrá, ya que podemos plantear la función afín
    como un sistema de ecuaciones lineales de $r+1$ incógnita y 2 soluciones.}
    Por lo que $G(A(x)) \neq G(A(y))$ y tenemos como buscábamos que $\pmcg$ separa los puntos de $K$. 

    Veamos finalmente que para todo punto de $k$ existe una función de $\pmcg$  en el que la imagen no es nula.  

    Por ser $G$ no constante volvemos a tomar un $a \in \R$ tal que $G(a) \neq 0$ , consideramos ahora la aplicación lineal
    $A(x) = 0 \cdot x + a$ por lo que para todo $x \in K$, $G(A(x)) = G(b) \neq 0$. 

    Como hemos comprobado se verifican todas las hipótesis del teorema de Stone-Weierstrass, con lo que concluimos que 
    la clausura uniforme de $\pmcg$ en $\R$ consiste en todas las funciones continuas de variable real en un compacto,
     o equivalentemente, $\pmcg$ es denso en el espacio denso en el  espacio de la funciones continuas de una variable en un compacto. 
\end{proof}

\subsection{Observaciones y reflexiones sobre el teorema de convergencia real en compactos}

Con esto lo que acabamos de probar que \textit{feedforward neural networks} con tan solo una capa oculta son $\sum \prod$ son capaces de aproximar cualquier 
función continua de variable real en un compacto.  Cabe destacar que a la función $G$, la función de activación, solo se le ha pedido como 
hipótesis ser una función continua de variable real. \textcolor{red}{¿Por qué pide entonces tales características en la definición de 
función de activación?}    

Además, solo se está demostrando para le caso de una capa oculta, como veremos a continuación es fácilmente generalizable a redes neuronales 
con varias capas ocultas, sin embargo; esto pone de manifiesto, si se quieren formular nuevos teoremas en el campo de las redes neuronales
multicapas a la necesidad de una formulación más abstracta de las mismas. 

Además es fácil ver generalizaciones del método. 
%%% Corolarios propios 

\textcolor{red}{Estos corolarios los he pensado por mi cuenta, OJO con ellos por si he metido la bacalá}

Notemos que la función de activación $G$ es única en toda la estructura,
sin embargo es habitual la combinación de éstas en una misma red neuronal. 

\begin{corolario}[Pueden combinarse distintas funciones de activación en una misma red neuronal]

    Una misma red neuronal puede estar constituida por una familia de funciones continuas no constantes $\Gamma$, 
    bastará con generalizar $\pmcg$ a $\sum \prod ^r (\Gamma)$ donde 
    \begin{equation}
        \begin{split}
            \sum \prod (\Gamma) = \{ 
                &f: \R^r \longrightarrow \R | \\
                & f(x) = \sum_{j = 1} ^q  \beta_j \cdot \prod_{k=1}^{l_j}
                G(A_{jk}(x)), \\
                &x  \in \R^r, \beta_j \in \R, A_{jk}\in A^r, l_j,q \in \N, G \in \Gamma
                )
                \}
        \end{split}
    \end{equation}
\end{corolario}
\begin{proof}
    La demostración es idéntica a la dada en el Teorema de convergencia 
    real en compactos \ref{teo:TeoremaConvergenciaRealEnCompactosDefinicionesEsenciales}
\end{proof}

Notemos que este resultado no da pista alguna de las ventajas de una función frente a otra,
 ni cómo afecta a la \textit{velocidad de convergencia}. 

\begin{corolario}[Extensión a múltiples capas ocultas]

    Sea $\Gamma$ cualquier familia de funciones continuas definidas de $\R$ en $\R$. 
    Se tiene que $\sum \prod ^r (\Gamma)$ es uniformemente denso por compactos en $C^r$  
    Es decir, las redes neuronales con varias capas son densas en el  espacio de la funciones continuas de una variable en un compacto. 
\end{corolario}
\begin{proof}
    \textcolor{red}{Explicado muy intuitivamente, ya que habría que tener en cuenta la conexión de las neurona 
    capa por capa, por ejemplo ¿Es densamente conexa? ¿Recurrente? ¿convolucional? ¿una GAN?}

    Como con una capa ya se nos asegura la convergencia bastará con asegurar que exista 
    en el espacio de las redes neuronales profundas capas que transmitan la información sin cambiarla. 

    Una vez concretada la estructura de la red neuronal,  su estructura algebraica podría permitir esa transmisión. 
\end{proof}

Recordemos que de manera general se ha definido $A$ como una función afín 
$A(x) = w \cdot x + b$ donde $x$ y $w$ son vectores de $\R^r$ y $\cdot$ representa el producto 
usual de escalares y $b \in \R$ es un escalar.  ¿Pero que ocurriría si trabajáramos con transformaciones más generales?  
Por ejemplo $B((x_1, ..., x_r)) = \sum_{i= 0} ^N \sum_{j= 0} ^r \alpha_{ij} x_j^i$  con $N$ natural positivo. 

\begin{corolario}[Generalización de A]  

    Se puede extender $A^r$ a conjuntos más generales como el de los polinomios de $r$ variables de grado $N$, $\mathbb{P}$.  
\end{corolario}
\begin{proof}
    Simplemente hay que reparar que $A^r$ está contenido en el espacio $\mathbb{P}$. 
    Es más observando la demostración bastará con utilizar cualquier conjunto que contenga a $A^r$. 
\end{proof}

La utilidad de este corolario a nivel práctico es cuestionable, ya que aumentaría considerablemente el número de 
parámetros que ajustar de la red neuronal ocasionando: (1) la necesidad de mayor número de datos que aprender, 
(2) mayor costo computacional, (3) probablemente peores resultados a igual número de iteraciones en comparativa 
con otros modelos de menor número de neuronas (ya que el espacio de búsqueda ha aumentado).





