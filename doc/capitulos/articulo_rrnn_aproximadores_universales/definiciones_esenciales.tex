% !TeX root = ../../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Introducción artículo MFNAUA
%*******************************************************


\section{Definiciones esenciales}  

Se pretende con las siguientes definiciones una concepción y trabajo preciso de la clase de redes neuronales multicapa feedforward. 


\begin{definicion} Función de activación \\
    Una función  $\psi: \R \longrightarrow [0,1]$ es una \textbf{ función de activación} si  cumple las siguientes propiedad
    \begin{enumerate}[label=(\roman*)]
        \item Es no decreciente
        \item $\lim _{x \rightarrow \infty} \psi(x) = 1
        $.
        \item $\lim _{x \rightarrow -\infty} \psi(x) = 0$.
    \end{enumerate}  

    Las funciones de activación son medibles ya que tienen como mucho un número contable de discontinuidades.
   
    Ejemplos comunes son
    \begin{itemize}
        \item Funciones umbral. 
        (Vale cero hasta que se cumple cierta condición de las entradas, a partir de ahí empezaría a valer uno).

        \item Funciones indicadoras: $\psi(\lambda) = 1_{\{\lambda > 0\}}$. 
        \item Ramp function: $\psi(\lambda)  = \lambda 1_{\{0 \leq \lambda \leq  1\}} + 1_{\{\lambda > 1\}}$
    
        \item La función \textit{cosine squasher} de Gallant and White (1988)
        \begin{equation*}
    \psi(lambda )= (1 + cos(\lambda + 3 \frac{\pi}{2}) \frac{1}{2}) 
     1_{\{\frac{-\pi}{2} \leq \lambda \leq  \frac{\pi}{2}\}}
     1_{\{ \frac{\pi}{2} < \lambda \}}
    \end{equation*}
    \end{itemize}

    Cabe destacar que la definición tomada es la propuesta en \cite{HORNIK1989359} y que en otros casos 
    hay quien acepta funciones otras posibles definiciones menos restrictivas.
\end{definicion}

\begin{definicion} Función afín

    Para cualquier natural mayor que cero $r$  denotaremos por $A^r$ como al conjunto de todas 
    las funciones afines de $\R^r$ a $\R$. Es decir el conjunto de funciones de la forma 
    $A(x) = w \cdot x + b$ donde $x$ y $w$ son vectores de $\R^r$ y $\cdot$ representa el producto 
    usual de escalares y $b \in \R$ es un escalar.  
    
\end{definicion}  

En este contexto, $x$ corresponde al vector entrada de la red neuronal, $w$ los pesos de la red
que se multiplicarán con $x$ en la capa intermedia y $b$ el sesgo. 

Nótese que faltaría componer la función afín con una función de activación para obtener lo que hemos definido 
como un perceptrón. 

\begin{definicion} Primera formalización red neuronal multicapa \\
    Para cualquier función Borel medible $G(\cdot)$, definida de $\R$ a $\R$ y cualquier natural positivo ç
    $r \in \N$ de define a la clase de funciones $\pmc$ como 
    \begin{equation}
        \begin{split}
        \pmc = 
        \{ 
            & f: \R ^r \longrightarrow \R \\
            &f(x)=\sum_{j = 1} ^q (
            \beta_j G(A_{j}(x)), \\
            & x  \in \R ^r, \beta_j \in \R, A_{j}\in A^r, l_j,q \in \N
            )
        \}
        \end{split}
    \end{equation}
\end{definicion}


Obsérvese que cada sumatoria representa un perceptrón y la salida final 
es una combinación lineal de los distintos perceptrones. 

Con esta misma idea se definirá la siguiente estructura que generaliza a la familia $\pmc$  
   
\begin{definicion} Generalización de la familia $\pmc$ \\
    
    \begin{equation} 
        \begin{split}
        \sum \prod^r(G) = \{ 
        &f: \R^r \longrightarrow \R | \\
        & f(x) = \sum_{j = 1} ^q , \beta_j \cdot \prod_{k=1}^{l_j}
        G(A_{jk}(x)), \\
        &x  \in \R^r, \beta_j \in \R, A_{jk}\in A^r, l_j,q \in \N
        )
        \}
    \end{split}
    \end{equation}  

    Notemos que $\pmc$ es el caso particular $l_j = 1$ para todo $j$. Nuestro primer
    resultado se demostrará para $\sum \prod^r(G)$ y como consecuencia quedará probado
    también para $\pmc$. 
\end{definicion}


\subsection{ Reflexión sobre la relevancia de la función de activación}  

La función de activación acota la imagen de un una aplicación afín, esta limitación 
es de total relevancia ya que permite una interpretación previamente convenida. 
Decimos por ende que es la causante del \textit{aprendizaje}.     

Por si mi comentario no ha resultado claro procedo a dar un ejemplo:   
Supongamos que nos hallamos frente a un problema de clasificación de dos clases y que la función de activación
tomada es una función umbral que toma valores cero o uno. Define por tanto esta función una separación de clases. 
De otra manera, de solo existir la función afín sin una transformación de la salida, el codominio serían los Reales
donde a priori no se explicita una asignación de clase.  

Introduciremos a continuación notación de los conjuntos de funciones que seremos capaces de aproximar.  

\begin{definicion} Conjuntos de funciones continuas y Borel-medibles \\
    Sea $C^r$ el conjunto de funciones continuas con dominio en $\R^r$ y codominio $R$, 
    y sea $M^r$ el conjunto de todas las funciones Borel medible de $\R^r$ a $\R$. 

    Denotaremos también a la $\sigma$-álgebra de Bore en $\R ^r$ como $B^r$. 
\end{definicion}

En lo que respecta a definiciones anteriores, $\pmc$ y $\pmcg$ pertenecen a 
$M^r$ para cualquier función Borel-medible $G$. Si $G$ es continua entonces 
$\pmc$ y $\pmcg$ pertenecen a $C^r$. Tengamos presente que $C^r$ es un subconjunto ç
de $M^r$.  

Como veremos más adelante primero demostraremos el teorema para $C^r$ y después 
extenderemos el resultado para $M^r$.   


\subsection{ Reflexión sobre el tipo de conjuntos que se pueden aproximar}