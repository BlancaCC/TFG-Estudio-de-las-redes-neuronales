% !TeX root = ../../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Contenido del artículo 1: Definiciones primeras
%*******************************************************


\section{Definiciones primeras}\label{ch:articulo:sec:defincionesPrimeras}  

Se pretende con las siguientes definiciones una concepción y trabajo preciso de la clase de redes neuronales multicapa feedforward. 
Se precisa nociones como la de función de activación \ref{def:funcion_activacion_articulo}, 
se abstrae y formaliza matemáticamente la estructura de una red neuronal \ref{def:articulo_abstracción_rrnn} 

\begin{definicion}[Función de activación ] \label{def:funcion_activacion_articulo}
    Una función  $\psi: \R \longrightarrow [0,1]$ es una \textbf{ función de activación} si  cumple las siguientes propiedades:
    \begin{enumerate}[label=(\roman*)]
        \item Es no decreciente.
        \item $\lim _{x \rightarrow \infty} \psi(x) = 1
        $.
        \item $\lim _{x \rightarrow -\infty} \psi(x) = 0$.
    \end{enumerate}  

    Las funciones de activación son medibles ya que tienen como mucho una cantidad numerable de discontinuidades.
   
    Ejemplos comunes de funciones de activación son
    \begin{itemize}
        \item Funciones umbral. 

        \item Funciones indicadoras: $\psi(\lambda) = 1_{\{\lambda > \lambda_0\}}$ con $\lambda_0 \in \R$. 
        \item Función rampa: $\psi(\lambda)  = \lambda 1_{\{0 \leq \lambda \leq  1\}} + 1_{\{\lambda > 1\}}.$
    
        \item La función \textit{cosine squasher} de Gallant and White (1988)
        \begin{equation*}
    \psi(\lambda )= \left(1 + cos\left(\lambda + 3 \frac{\pi}{2} \right) \frac{1}{2}\right) 
     1_{\{\frac{-\pi}{2} \leq \lambda \leq  \frac{\pi}{2}\}}
     +
     1_{\{ \frac{\pi}{2} < \lambda \}}.
    \end{equation*}
    \end{itemize}

    Cabe destacar que la definición tomada es la propuesta en \cite{HORNIK1989359} y que existen
    otras posibles definiciones menos restrictivas.
\end{definicion}



    Para cualquier natural $r$ mayor que cero  denotaremos por $A^r$ al conjunto de todas 
    las \textbf{funciones afines} de $\R^r$ a $\R$. Es decir el conjunto de funciones de la forma 
    $A(x) = w \cdot x + b$ donde $x$ y $w$ son vectores de $\R^r$,  $b \in \R$ es un escalar y $\cdot$ representa el producto 
    usual de escalares.  
    


En este contexto, $x$ corresponde al vector entrada de la red neuronal, $w$ los pesos de la red
que se multiplicarán con $x$ en la capa intermedia y $b$ el sesgo. 

Nótese que faltaría componer la función afín con una función de activación para obtener lo que hemos definido 
como un perceptrón. 

\begin{definicion} [Primera  aproximación a la formalización de una red neuronal]
    Para cualquier función Borel medible $G$, definida de $\R$ a $\R$ y cualquier natural positivo
    $r \in \N$ se define a la clase de funciones $\pmc$ como 
    \begin{equation}
        \begin{split}
        \pmc = 
        \{ 
            & f: \R ^r \longrightarrow \R / \quad
            f(x)=\sum_{j = 1} ^q (
            \beta_j G(A_{j}(x)), \\
            & x  \in \R ^r, \beta_j \in \R, A_{j}\in A^r,q \in \N
            )`'
        \}.
        \end{split}
    \end{equation}
\end{definicion}


Obsérvese que cada elemento de la sumatoria representa un perceptrón y que la salida final 
es una combinación lineal de los distintos perceptrones. Con esta misma idea 
se define la siguiente estructura que generaliza a la familia $\pmc.$  
   
\begin{definicion} [Formalización de una red neuronal con una capa oculta]\label{def:articulo_abstracción_rrnn}
    
    \begin{equation} 
        \begin{split}
        \sum \prod^r(G) = \{ 
        &f: \R^r \longrightarrow \R /\\
        & f(x) = \sum_{j = 1} ^q  \beta_j \prod_{k=1}^{l_j}
        G(A_{jk}(x)), \\
        &x  \in \R^r, \beta_j \in \R, A_{jk}\in A^r, l_j,q \in \N
        )
        \}.
    \end{split}
    \end{equation}  

    Notemos que $\pmc$ se recupera en el caso particular en el que $l_j = 1$ para todo $j$.
    Los elementos de $\pmcg$ son combinaciones lineales de productos finitos de perceptrones. 

\end{definicion}


\subsection{ Reflexión sobre la relevancia de la función de activación}  

La función de activación $G$ es clave en el proceso de aprendizaje.
Por una parte nótese que como $A^r$ es un espacio afín, lo que se está haciendo es 
aproximar una función medible a partir de combinaciones lineales de \textit{rectas} evaluadas mediante $G$. 
 
\begin{figure}[h!]
    \includegraphics[width=\textwidth]{articulo_rrnn_aproximadores_universales/ejemploAproximaciónCurvasPorRectas.jpeg}
    \caption{Ejemplo de curva aproximada por perceptrones multicapa \cite{alma991008058419704990}.}
    \label{img:def_esenciales_ejemplo_curva_aproximada_percentrón_multicapa}
\end{figure}

Por otra parte, la función de activación acota la imagen de un una aplicación afín, esta limitación 
es de total relevancia ya que permite una interpretación previamente convenida. 
Decimos por ende que es la causante del \textit{aprendizaje}.     

Por si el comentario no ha resultado claro procedamos a dar un ejemplo:   
Supongamos que nos hallamos frente a un problema de clasificación de dos clases y que la función de activación
tomada es una función umbral que toma valores cero o uno. Define por tanto esta función una separación de clases. 
De otra manera, de solo existir la función afín sin una transformación de la salida, el codominio serían 
todos los número reales donde a priori no se explicita una asignación de clase.  

Introducimos a continuación la notación de los conjuntos de funciones que seremos capaces de aproximar.  

Denotamos por  $\fC$ al conjunto de funciones continuas con dominio en $\R^r$ y codominio $\R$,
por  $\fM$ al conjunto de todas las funciones Borel medible de $\R^r$ a $\R$. 
y por $B^r$ a la $\sigma$-álgebra de Borel en $\R ^r$. 

En lo que respecta a definiciones anteriores, $\pmc$ y $\pmcg$ pertenecen a 
$\fM$ para cualquier función Borel-medible $G$. Si $G$ es continua entonces 
$\pmc$ y $\pmcg$ pertenecen a $\fC$. Tengamos presente que $\fC$ es un subconjunto
de $\fM$.  
  

\subsection{ Reflexión sobre el tipo de funciones que se pueden aproximar}

La existencia de funciones no medibles manifiesta una limitación
de la formalización actual de las redes neuronales que plantea las siguientes 
preguntas: 
\begin{enumerate}
    \item ¿Supone la existencia de este tipo de funciones una verdadera limitación a nivel práctico?
    \item ¿Se podría construir alguna arquitectura que sí que las aproximara?
\end{enumerate}  

Continuando con el hilo de la segunda cuestión, si se carece de un espacio vectorial, 
de una medida,  ¿Cómo se podría construir una sucesión de funciones que se aproxime?
Quizás habría que buscar características más intrínsecas del problema en cuestión, 
razonamientos topológicos.

\begin{definicion} [Subconjunto denso ] 
    Dado un subconjunto $S$ de un espacio métrico $(X, \rho)$, se dice que $S$ es denso por la distancia $\rho$
    en subconjunto $T$ si para todo $\epsilon$ positivo y cualquier $t \in T$ existe un $s \in S$ tal 
    que $\rho(s,t) \leq \epsilon$. 
\end{definicion}

Un ejemplo habitual sería en el espacio métrico $(\R, |\cdot|)$ con $|\cdot|$ el valor absoluto, el subconjunto 
$T = \R$ y $S$ los números irracionales, $S = \R \setminus \Q$. 

A nivel intuitivo sería decir que los elementos de $S$ son capaces de aproximar cualquier elemento de $T$
con la precisión que se desee. 

\begin{definicion} 
    Un subconjunto $S$ de $\fC$ se dice que es \textbf{uniformemente denso para compactos} en  $\fC$
    si para cada subconjunto compacto $K \subset \R^r$ se tiene que $S_K$ es denso según $\rho_K$ en $\fC$
    donde $\rho_K$ está definida como sigue.
    Para cualquier $f,g \in \fC$ 
    \begin{equation}
        \rho _ K (f,g) = \sup_{x \in K} |f(x) - g(x)|.
    \end{equation}
\end{definicion}

\begin{definicion}
    Una serie de funciones $\{f_n\}$ \textbf{converge uniformemente a una función $f$ sobre compactos} si para 
    cada  conjunto compacto $K \subset \R^r$  se cumple que
    \begin{equation}
        \rho_k (f_n, f) \longrightarrow 0 \text{ cuando } n \longrightarrow \infty.
    \end{equation} 
\end{definicion}

