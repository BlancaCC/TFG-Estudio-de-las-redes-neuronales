% !TeX root = ../../tfg.tex
% !TeX encoding = utf8
%
%***************************************************************
% Contenido del artículo 5: Colorarios LP
%***************************************************************
\section{Generalización a espacios $L_p$}  

Hasta ahora habíamos considerado el espacio de funciones continuas 
$\fC$ 
como subespacio dentro del espacio de funciones medibles $\fM$. 

Sin embargo ser continua es una hipótesis muy estricta que queda
notable en la amplia gama de subespacios que contienen al de 
las funciones continuas y están contenidos en el de funciones medibles. 

Es por ello que vamos a realizar una generalización de los teoremas
para espacios $L_p$. De manera intuitiva estos espacios nos van a 
permitir considerar funciones que no necesariamente sean continuas
y que incluso puedan \textit{tomar} valores puntuales \textit{que sean}
$\infty$ o $- \infty$. 

\begin{definicion}[Espacios Lp]
    Se llama espacio $L_p(\R^r, \mu)$ o simplemente $L_p$ al conjunto 
    de funciones $f \in \fM$ tales que 
    \begin{equation}
        \int |f(x)|^p d\mu < \infty. 
    \end{equation}

Se define la norma de $L_p$ como 
\begin{equation}
    \| f\|_p 
    =
    \left(\int |f(x)|^p d\mu \right)^\frac{1}{p}.
\end{equation}

La distancia asociada al espacio $L_p$ se define como 
\begin{equation}
    \rho_p(f,g) = \| f-g\|_p.
\end{equation}
\end{definicion}

% Corolario 2.2
\begin{corolario}\label{corolario:2_2_rrnn}
    Si existe un subconjunto compacto $K$ en $\R^r$ de medida
    $\mu(K) =1$ entonces $\rrnn$ es $\dlp$-denso en $L_p(\R^r, \mu)$
    para cualquier $p \in [1,\infty)$, independientemente de 
    $\psi$, $r$ o $\mu$.
\end{corolario}
\begin{proof}
    Se quiere probar que para cualquier $g \in L_p$ y 
    $\epsilon >0$ existe un $f \in \rrnn$ tales que 
    \begin{equation}
        \dlp(f,g) <\epsilon.
    \end{equation}   
    
    Por pertenecer $g$ a $L_p$ existe una constante $M$ real positiva
    los suficientemente grande 
    tal que si definimos la función $h =g 1_{|g|<M}$ esta satisface 
    que
    \begin{equation}\label{eq:corolario_2_2:h_compacto}
        \dlp(g,h) < \frac{\epsilon}{3}.
    \end{equation}
    
    Además como $h$ es una función acotada de $L_p$, podemos encontrar
    una función $s$ continua que es límite de una sucesión de
    funciones simples 
    ( pag 241-242,  teoremas 55C y 55D \cite{nla.cat-vn1819421})
    y la cual cumple que 

    \begin{equation}\label{eq:corolario_2_2:s_continua}
        \dlp(h,s) < \frac{\epsilon}{3}.
    \end{equation}

    Por el teorema \ref{teo:2_4_rrnn_densas_M}, al estar en un compacto $K$ y por ser $\rrnn$ uniformemente
    denso en compactos hay una $f \in \rrnn$ la cual cumple que
    \begin{equation}
        \sup_{x \in K} |f(x) -s(x)|^p 
        <
         \left( \frac{\epsilon}{3}\right) ^p.
    \end{equation}
    
    Y por hipótesis $\mu(K) =1$ y definición de la distancia $\dlp$ 
    se tiene la siguiente desigualdad: 

    \begin{equation} \label{eq:corolario_2_2:cota_rrnn}
        \dlp(f,s) = 
        \left(\int |f(x) - s(x)|^p d\mu \right)^\frac{1}{p}
        \leq 
        \left(\int  \left( \frac{\epsilon}{3}\right) ^p d\mu \right)^\frac{1}{p}
        = \left( \mu(K)  \left(\frac{\epsilon}{3} \right)^p\right) ^\frac{1}{p}
        = \frac{\epsilon}{3}.
    \end{equation}

    Gracias a la desigualdad triangular y las desigualdades
    (\refeq{eq:corolario_2_2:cota_rrnn})
    (\refeq{eq:corolario_2_2:h_compacto})
    (\refeq{eq:corolario_2_2:s_continua})

    \begin{equation}
        \dlp(f,g) 
        \leq
            \dlp(f,s)
            +\dlp(s,h)
            + \dlp(h,g)
        < 
        \frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3}
        = \epsilon.
    \end{equation}
Probando con ello lo buscado. 
\end{proof}  

% Corolario 2.3
\begin{corolario}
    Si $\mu$ es una medida de probabilidad en $[0,1]^r$
    entonces 
    $\rrnn$ es $\dlp$-denso en 
    $L_p([0,1]^r, \mu)$ para todo $p \in [1, \infty)$,
    independientemente de $\psi, r, \mu$. 
\end{corolario}
\begin{proof}
    Es consecuencia directa del corolario previo \ref{corolario:2_2_rrnn}
    donde para este caso particular $K = [0,1]^r$ un compacto
    de $\R^r$
    que cumple que $\mu(K) = 1.$
\end{proof}

%Corolario 2.4 
\begin{corolario} \label{corolario:2_4_conjunto_finito}
    Sea $\mu$ una una medida que evalúa a uno un conjunto 
    finito de puntos, 
    entonces, para cualquier función medible $g \in \fM$
    y sea cual sea $\epsilon >0$ 
    existe $f \in \rrnn$ la cual cumple que 
    \begin{equation}
        \mu\{ 
            x:
            |f(x) - g(x)| 
            < \epsilon
        \}
        = 1.
    \end{equation}

\end{corolario}
\begin{proof}
    Por el teorema \ref{teo:2_4_rrnn_densas_M} existirá una 
    $f \in \rrnn$ tal que para cualquier 
    $\epsilon_1, \epsilon_2 >0$ se cumpla que 
    $\mu \{x: |f(x) - g(x)| > \epsilon_1\} < \epsilon_2.$
    Sea $O$ el conjunto de puntos tal que $\mu(O) = 1.$
    Por ser finito $O$ podemos encontrar
    \begin{equation} \label{eq:2_4:definición_epsilon}
        \delta = \min_{x \in O} \{ 
            \mu(x) : \mu(x)>0
        \}. 
    \end{equation}

    Sin pérdida de generalidad tomamos $\epsilon < \delta$ y entonces
    para cualquier $f$ que cumpla que 
    \begin{equation}
        \dist(f,g) = \epsilon
    \end{equation}
    por cómo se define la distancia de una medida
    \begin{equation}
        \dist(f,g) =  \inf 
        \{
           \epsilon > 0:
           \mu\{ 
            x:
            |f(x) - g(x)| 
            > \epsilon
        \}
        < \epsilon
        \}
    \end{equation}
    y la forma de tomar $\epsilon$ en (\refeq{eq:2_4:definición_epsilon}) 
    tiene que $\mu\{ 
        x:
        |f(x) - g(x)| 
        > \epsilon
    \} = 0.$

    Es decir probando con ello que 
    \begin{equation}
        \mu\{ 
            x:
            |f(x) - g(x)| 
            < \epsilon
        \}
        = 1.
    \end{equation}
\end{proof}

Nótese que con este corolario lo que se está indicando es que en 
conjunto finito se puede encontrar una red neuronal
converge de manera exacta en esos puntos. 

\begin{definicion}[Función Booleana]
    Sea 
    \begin{equation}
        B^r = \{
            (x_1, ..., x_r) : x_i \in \{0,1\}, 
            \text{ para todo } i \in \{1,..., r\}
            \}.
    \end{equation}

    Se dice $f$ es una función Booleana 
    si formalmente esta definida de $f:B^r \longrightarrow B$. 
\end{definicion}
Ejemplo conocidos son la función $or: B^r \longrightarrow B$  que vale 
uno si alguno de su entrada es uno y la función 
$and: B^r \longrightarrow B$
que se define como $and(x_1, ..., x_r) = \prod_{i=1}^r x_i.$

% Corolario 2.5  
\begin{corolario}
    Para cada función Booleana $g: B^r \longrightarrow B$ y 
    cada $\epsilon >0$ existe una red neuronal
    $f \in \rrnn$ tal que 
    \begin{equation}
        \max_{x \in \{ 0,1\}^r} |g(x) - f(x)|
        < \epsilon.
    \end{equation}
\end{corolario}
\begin{proof}
    Se define la función $\mu : \R^r \longrightarrow [0,1]$ de la forma que 
    \begin{equation}
        \mu(x) = 
      \left \{
    \begin{aligned}
      \frac{1}{2^r} \quad &\text{ si } x \in B^r \\
      0 \quad & \text{ si } x \notin B^r 
    \end{aligned}
  \right .
    \end{equation}

    Se tiene que $\mu$ es una medida ya que cumple que 
    \begin{enumerate}
        \item Hipótesis de acotación: $0 \leq \mu(A) \leq 1$ para $A \in \mathcal{P}(\R^r).$
        \item Probabilidad del vacía es nula y la del  total la unidad. 
        \item La probabilidad de la unión es la suma de la probabilidades. 
        \begin{equation}
            P\left(
                \cup_{i=1}^n A_i
            \right)
            = \sum_{i=1}^n P(A_i).
            \quad
            \forall A_i \in  \mathcal{P}(\R^r).
        \end{equation}
    \end{enumerate}  

    Como la cardinalidad de $B^r$ es $2^r$, es decir finita
    podemos aplicar el corolario \ref{corolario:2_4_conjunto_finito}
    y entonces sabemos que  existe una $f\in \rrnn$ tal que 
    \begin{equation}
            \mu\{ 
                x:
                |f(x) - g(x)| 
                < \epsilon
            \}
            = 1,
    \end{equation} 
    es decir que 
    \begin{equation}
        \max_{x \in \{ 0,1\}^r} |g(x) - f(x)|
        < \epsilon
    \end{equation}
    como queríamos probar. 
\end{proof}
% Lemas propios  previos al teorema 2.5
\begin{lema} \label{lema:propio_1_antes_teorema_2_5}
    Si una función de activación  $\psi$ alcanza el cero y el uno, esto es 
    si existen dos constantes reales $M_1, M_2$ 
    tales que 
    \begin{equation}
        \psi(M_1) = 0 \text{ y } \psi(M_2)=1
    \end{equation}
    entonces existe una constante real positiva $M$ tal que 
    \begin{equation}
        \psi(-M) = 1- \psi(M) = 0.
    \end{equation}
\end{lema}
\begin{proof}
Sea $M = \max \{|M_1|,|M_2|\}$ y por ser $\psi$ una función de activación sabemos que
es no decreciente y que su imagen pertenece al intervalo $[0,1].$

Por tanto
\begin{align}
      0 &\leq \psi(-M) \leq \psi(M_1) = 0 \quad \text{ luego } \quad \psi(-M) = 0, \\
      1 &\geq \psi(M) \geq \psi(M_2) = 1 \quad \text{ luego } \quad\psi(M) = 1
\end{align}

Gracias a estas desigualdades es fácil ver que 
\begin{equation}
    \psi(-M) = 1 - \psi(M) = 0
\end{equation}
como queríamos probar. 
\end{proof}   

Es interesante percatarse de que de no exigirse la hipótesis de 
que $\psi$ alcanza el cero o el uno o que no es continua no puede 
asegurarse la igualdad demostrada. Pongamos como ejemplo la siguiente función de activación

\begin{equation}
    \psi(x)= \left\{ \begin{array}{lcc}
        0 &   si  & x \leq 0 \\
        \frac{\lceil x \rceil}{1+ \lceil x \rceil}&  si & 0< x  
        \end{array}
    \right. 
\end{equation}

% Otro lema propio antes de probar el teorema 2.5
\begin{lema}\label{lema:previo_propio_2_al_teorema_2_5}
    Dado un conjunto finito de vectores $\Lambda \subset \R^r$ con 
    $r$ natural mayor que uno. 
    Se tiene que existe un vector $p \in \R^r$ que satisface que 
    para cualesquiera $x,y \in \Lambda$ diferentes 
    \begin{equation}
        p \cdot(x-y) \neq 0.
    \end{equation}
\end{lema}

% Teorema 2.5  
\begin{teorema}[Sobre el entrenamiento práctico de redes neuronales]
    Sea $ \Lambda = \{x_1, \ldots, x_n\}$ un conjunto de puntos distintos de 
    $\R^r$ y sea 
    $g: \R^r \longrightarrow \R$ una función arbitraria. 
    Si $\psi$ alcanza el cero y el uno, 
    entonces
    existe una red neuronal $f \in \rrnn$ con $n$
    capas ocultas tal que 

    \begin{equation}
        f(x_i) = g(x_i) \text{ en todo } i \in \{1, \ldots, n \}.
    \end{equation}
\end{teorema}
\begin{proof}
Construiremos la demostración en tres casos. 

\textbf{Caso primero}

Suponemos que $\{x_1, \ldots, x_n\} \subset \R$ y tras renombrar 
podemos suponer que $x_1 < x_2 < \ldots < x_n.$ 
Por alcanzar la función de activación $\psi$ el cero y el uno, 
gracias al lema  \ref{lema:propio_1_antes_teorema_2_5} existe una constante $M$ tal que $\psi(-M) = 1-\psi(M) = 0.$

Definiremos de manera recursiva a la red neuronal. 

\begin{itemize}
    \item Red neuronal $f_1$. 

Sea $A_1$ la función afín constante $A_1 = M.$
Fijamos $\beta_1 = g(x_1)$. 
De esta manera la red neuronal $f_1$ 
definida como $f_1(x) = \beta_1 \psi(A_1(x)).$

\item Red neuronal $f_k$ con $1 < k \leq n$. 

Se define $A_{k}$ como la única función afín que cumple que 
\begin{equation}
    A_k(x_{k-1}) = -M \quad \text{y} \quad  A_{k}(x_k)= M.
\end{equation}
Fijamos $\beta_k = g(x_k) - g(x_{k-1})$. 
La red neuronal $f_k$ se calcula como 
\begin{equation}
    f_k(x) 
    = 
    \sum_{j=1}^k \beta_j \psi(A_j(x))
     = 
    (g(x_k)-g(x_{k-1})) \psi(A_k(x)) + f_{k-1}(x) .  
\end{equation}
Observemos que así construida se tiene que para cualquier
 $y > x_k$ la evaluación con la red neuronal resulta $f_k(y) = g(x_k).$

\end{itemize}
Veamos por inducción sobre $n$ que así definida para cualquier $1 \leq i \leq n$ se tiene que     
$f_n(x_i) = g(x_i)$. 


\begin{itemize}
    \item Caso base, $n=1$. 
    \begin{equation}
        f_1(x_1)= \beta_1 \psi(A_1(x_1)) = g(x_1)\psi(M) = g(x_1).
    \end{equation}
    \item Supuesto que es cierto para $n-1$ veamos que lo es para $n$.      
    Evaluación de $x_n$
    \begin{align}
        f_n(x_n) 
        &= 
        (g(x_n) - g(x_{n-1}))\psi(A_n(x_n)) + f_{n-1}(x_n)
        \\
        & = (g(x_n) - g(x_{n-1}))\psi(M) + g(x_{n-1}) 
        \\
        & = (g(x_n) - g(x_{n-1})) + g(x_{n-1}) 
        \\
        & = g(x_n).
    \end{align}

Evaluación de $x_i$ con $1 \leq i < n$. 

Usando que $0 \leq \psi(A_n(x_i)) < \psi(A_n(-M)) = 0$ y la hipótesis de inducción se tiene que 
\begin{align}
    f_n(x_i) 
        &= 
        (g(x_n) - g(x_{n-1}))\psi(A_n(x_i)) + f_{n-1}(x_n)
        \\
        & = 0 + g(x_{i}) 
        \\
        &= g(x_i).
\end{align}
\end{itemize}

Acabamos de probar por inducción 
\begin{equation}
    f_n(x) = g(x) \text{ para cualquier } x \in \Lambda. 
\end{equation}

Es decir, el caso primero que existe una red neuronal $f_n$ que dadas las hipótesis satisface la tesis en el caso que $\Lambda \subset \R.$

\textbf{Caso segundo}  

Se tiene para este caso que $\Lambda \subset \R^r$ con $r >1$. 
Seleccionamos un $p \in \R^r$ cumpliendo que 
para cualesquiera $x,y \in \Lambda$ distintos 
se cumpla que $p(x-y) \neq 0$. Este $p$ existe como hemos comprobado en el lema \ref{lema:previo_propio_2_al_teorema_2_5}.

Gracias a esto se puede establecer una relación de orden, que tras 
renombrar los elementos de $\Lambda$ queda
 $p \cdot x_1 < p \cdot x_2 < \ldots < p \cdot x_n.$

 Y como procedimos en el caso primero, definiremos de manera
 recursiva la red neuronal

 \begin{itemize}
 \item Red neuronal $f_1$. 

Sea $A_1$ la función afín constante $A_1 = M.$
Fijamos $\beta_1 = g(x_1)$. 
De esta manera la red neuronal $f_1$ 
definida como $f_1(x) = \beta_1 \psi(A_1(p \cdot x)).$

\item Red neuronal $f_k$ con $1 < k \leq n$. 

Se define $A_{k}$ como la única función afín que cumple que 
\begin{equation}
    A_k(p \cdot x_{k-1}) = -M 
    \quad \text{y} \quad 
     A_{k}(p \cdot x_k)= M.
\end{equation}
Fijamos $\beta_k = g(x_k) - g(x_{k-1})$. 
La red neuronal $f_k$ se calcula como 
\begin{equation}
    f_k(x) 
    = 
    \sum_{j=1}^k \beta_j \psi(A_j(p \cdot x))
     = 
    (g(x_k)-g(x_{k-1})) \psi(A_k(p \cdot x)) + f_{k-1}(x) .  
\end{equation}
\end{itemize}

Así definida, la prueba por inducción es idéntica a la mostrada 
en el caso primero probando con ello el teorema.

\end{proof}

%% Redactar mejor esta sección 
\textbf{Conclusión que falta aún por escribir bien}

Hasta ahora no tenemos ni idea una estimación del número de 
neuronas que necesitamos.  ¿son estas un número tan grande que 
resulte inasumible?
La idea que reside en este teorema es que si tenemos un conjunto
finito de puntos siempre se podrá encontrar una red neuronal 
que los evalúe de manera exacta. 

Este teorema nos muestra dos caras de una manera 
Aspecto bueno: 
Podremos reproducir un conjunto finito de puntos con una red neuronal e independientemente de la regla subyacente a estos. 
Malo: 
Supongamos que queremos entrenar una red neuronal para que prediga 
de acorde a una regla $f$, en la vida real no tenemos $f$, pero sí
pares de entradas e imágenes. Al ser el conjunto finito, 
esas mismas relaciones pueden ser consecuencia de una clase 
\textit{infinita no numerable} de funciones. 
Al \textit{entrenar} con un conjunto de punto finito  la red será capaz de adaptarse  a esos puntos y no 
necesariamente aproximar $f$. 

AÑADIR IMAGEN DE SPLINE Y POLINOMIO. 

Ante esta situación podríamos aumentar el número de datos, 
y suponiendo que *continuidad* podrían darse muchos casos distintos 

podría ser interesante visualizar la simpleza
Ockham -> si tengo pocos datos optaré por modelos simples. 

Puede dar el caso de que los datos tengan también errores o ruido 
al ser obtenidos, en ese caso estaremos aprendiendo fielmente mentiras. 

Haciendo el número de neuronas menos que N se podría conseguir. 

Nota para experimentar: quizás se podría ahorrar iteraciones de 
cálculo si utilizamos como \textit{backbone} (valores iniciales de los pesos) una red neuronal construida de esta manera. 
%% Hasta aquí conclusión que redactar mejor. 