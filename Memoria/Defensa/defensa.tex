
% Inbuilt themes in beamer
\documentclass{beamer}
% paquetes: 
\usepackage{mathpazo, amsmath, mathtools, }
\usepackage[spanish]{babel}
\usepackage[
backend=biber,
style=numeric,
sorting=ynt
]{biblatex}
% bibliografía
\addbibresource{bibliography.bib}

%%%%%%%%%%%%%%% comandos 
\newcommand{\R}{\mathbb{R}} 
\newcommand{\N}{\mathbb{N}}  
\newcommand{\Q}{\mathbb{Q}}  
\newcommand{\afines}{\mathcal{A}(\R^d)}
  \newcommand{\pmc}{\mathcal{H}_G(\R^d,\R)}%{\sum ^r (G)}  % Red neurona una capa una salida
  \newcommand{\pmcg}{ \sum \prod^d (G)} % Generalización red neuronal
  \newcommand{\fC}{\mathcal{C}(\R^d)} %conjunto de funciones continuas en R^r -> R
  \newcommand{\fM}{\mathcal{M}(\R^d)} % Conjunto funiones medibles
  \newcommand{\rrnn}{ \mathcal{H}(\R^d,\R)} % Red neuronal  sin subíndice
  \newcommand{\rrnng}{ \sum \prod^d (\psi)} % Red neuronal  generalizado
  \newcommand{\dist}{\rho_{\mu}}     % Distancia de una medida
  \newcommand{\dlp}{\rho_{p}} % Distancia de los espacios Lp
  % Múltiples salidas 
  \newcommand{\fCC}{\mathcal{C}(\R^d ,\R^s)}
  \newcommand{\fMM}{\mathcal{M}(\R^d , \R^s)}
  \newcommand{\rrnnmc}{ \mathcal{H}(\R^d,\R^s)} 
  \newcommand{\rrnnsmn}{ \mathcal{H}_n(\R^d,\R^s)} % Red neuronal salida múltiple con n neuronas
  \newcommand{\rrnngmc}{ \sum \prod^{d,s} (\psi)} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Datos generales
% Theme choice:
%\usetheme{Montpellier}
\usetheme{Boadilla}
\usecolortheme{rose}
\usefonttheme{structuresmallcapsserif}

% Title page details: 
\title{Optimización de redes neuronales} 
\author{Blanca Cano Camarero}
\date{30 de Junio de 2022}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
  % Title page frame
\begin{frame}
    \titlepage 
\end{frame}


% Outline frame
\begin{frame}{Índice}
    \tableofcontents
\end{frame}
% Presenta nuestro modelo 
\section{Introducción a las redes neuronales}
% Explicar que es un problema de aprendizaje automático supervisado
\begin{frame}{Usadas en problemas aprendizaje automático}
    Nos centraremos en problemas de aprendizaje automático supervisado. 

    Para el conjunto de datos  $\mathcal{D}$
    \begin{equation*}
        \mathcal{D} = \{(x,y) : x \in \mathcal{X}, y \in \mathcal{Y}\}
    \end{equation*}
    \begin{align*}
        &\text{¿Podemos construir } f: \mathcal{X} \longrightarrow \mathcal{Y} \\
        &\textit{tal que } f(x) = y \text{ para todo } (x,y) \in \mathcal{D} \text{?}
    \end{align*} 
\end{frame}
\begin{frame}{Definición redes neuronales}
    \begin{align*}
        \mathcal{H}(\R^d,\R^s) 
            =
            \{
                h &: \R^d \longrightarrow \R^s 
                 /\quad 
                h_k(x) = 
                \sum_{i=1}^{n} \beta_{i k} \gamma_{i}( A_{i}(x)), \\
                & \text{$h_k$  es la proyección k-ésima de $h$ con 
                $k \in \{1, \ldots, s\}$}, \\
                & n \in \N,\gamma_{i} \in \Gamma , \beta_{i k} \in \R \\
                &A_{i} \text{ una aplicación afín de $\R^d$ a $\R$}           
            \}.
    \end{align*}
\end{frame}

\begin{frame}{Comparativa de definiciniciones de red neuronal}
    Nuestra propuesta: 
    \begin{align*}
                h_k(x) = 
                %\textcolor{teal}{\underline{\hspace{0.4cm}}}
                \sum_{i=1}^{n} \textcolor{teal}{\beta_{i k}} \gamma_{i}( A_{i}(x))
    \end{align*}
    Modelo usual:
    \begin{align*}
        y_k(x) &= 
        \textcolor{red}{\theta_k}
        \left( 
            \sum^n_{j=1} \textcolor{red}{A_{k}}
            \left(
                \sigma 
                \left(
                    A^{d}_{j k}
                    \left(
                        x
                    \right)
                \right)
            \right)
        \right)
    \end{align*}
\end{frame}
\subsection{Modelo bien definido}
\begin{frame}{Las redes neuronales son aproximadores universales}
    \begin{block}{Teorema  \textbf{Las redes neuronales son una clase de aproximadores universales}}
        (Kurt Hornik, Maxwell Stinchcombe y Halber White 1989 \cite{HORNIK1989359})
        Una red neuronal con tan solo \textbf{una capa oculta} y con \textbf{una función de activación cualquiera} es capaz de \textbf{aproximar cualquier 
        función Borel medible}  con dominios y codominios de dimensión finita (no necesariamente iguales) y con el nivel de precisión que se desee \textbf{siempre y cuando 
        se utilicen suficientes neuronas}. En este sentido las redes redes neuronales son una clase de aproximadores universales.
    \end{block}
    En resumen: 
    \begin{equation*}
        \mathcal{H}(\R^d,\R^s)  \textit{ es denso en } \mathcal{M}(\R^d,\R^s) 
    \end{equation*}
\end{frame}
%%%%%%% salto de los números reales a los racionales...
\begin{frame}{Son computables}
    \begin{exampleblock}{Teorema}
        El espacio $\mathcal{H}(\Q^d, \Q^s)$ es denso en el espacio $\rrnnmc$. 
    \end{exampleblock}
\end{frame}

%% ¿Cómo podemos encontrar la red neuronal concreta 


\begin{frame}{Práctico}
    \pause
    \begin{itemize}
        \item Evaluar \textcolor{teal}{\checkmark}: \textit{forwardpropagation}.
        \item ¿Cómo se encuentra a la red neuronal?
    \end{itemize}
\end{frame}
\begin{frame}{Práctico}
    \begin{itemize}
        \item Evaluar \textcolor{teal}{\checkmark}: \textit{forwardpropagation}.
        \item Aprendizaje \textcolor{teal}{\checkmark}:  \textit{backpropagation}.
    \end{itemize}
    \pause 
    Nuestras adaptaciones además reducen el costo computacional
\end{frame}

\subsection{Equivalencia redes neuronales}
\begin{frame}{Práctico}
    Nuestra propuesta: 
    \begin{align*}
                h_k(x) = 
                \textcolor{teal}{\underline{\hspace{0.4cm}}}\sum_{i=1}^{n} \textcolor{teal}{\beta_{i k}} \gamma_{i}( A_{i}(x))
    \end{align*}
    Modelo usual:
    \begin{align*}
        y_k(x) &= 
        \textcolor{red}{\theta_k}
        \left( 
            \sum^n_{j=1} \textcolor{red}{A_{k}}
            \left(
                \sigma 
                \left(
                    A^{d}_{j k}
                    \left(
                        x
                    \right)
                \right)
            \right)
        \right)
    \end{align*}
    Solventamos diferencias: 
    \begin{itemize}
        \item $\theta$ eliminar observando convergencia en espacios $L_p$.
        \item ¿Se puede eliminar el sesgo?
    \end{itemize}
    (Sí y no ) jejejjej
\end{frame}
\begin{frame}{Bibliografía fundamental}
    \begin{itemize}
        \item Kurt Hornik, Marwell Stinchcombe y Halbert White. \textit{Multilayer feedforward networks are universal approximator}
    \end{itemize}
    
\end{frame}

%%%% 
%%% bibliografía
\begin{frame}
    \frametitle{Bibliografía}
    \printbibliography
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Diapositivas a posibles preguntas}

\begin{frame}{Posibles preguntas}
    \begin{itemize}
        \item Aproximador universal espacios intermedios: diapositiva \ref{univeral-aproximator}.
        \item Algunas definiciones: diapositiva \ref{definiciones-esenciales}.
    \end{itemize}
    
\end{frame}
\begin{frame}{Aproximador universal a funciones medibles}
    \label{definiciones-esenciales}
    \begin{align*} 
        \sum \prod^d(G) = \{ 
        &f: \R^r \longrightarrow \R / \quad
        f(x) = \sum_{j = 1} ^q  \beta_j \prod_{k=1}^{l_j}
        G(A_{jk}(x)), \\
        &x  \in \R^d, \beta_j \in \R, A_{jk}\in \afines; l_j,q \in \N
        \}.
    \end{align*} 
    Una función  $\psi: \R \longrightarrow [0,1]$ 
    es una \textbf{ función de activación} 
    si  cumple las siguientes propiedades:
    \begin{itemize}
        \item Es no decreciente.
        \item $\lim _{x \rightarrow \infty} \psi(x) = 1$.
        \item $\lim _{x \rightarrow -\infty} \psi(x) = 0$.
    \end{itemize}   
\end{frame}

\begin{frame}{Aproximador universal a funciones medibles}
    \label{univeral-aproximator}
    Relación ser denso en 
    \begin{align*}
        \rrnn 
            \Rightarrow  
        \rrnng 
            \Rightarrow
        \pmcg
            \Rightarrow  
        \fC    
            \Rightarrow 
        \fM .
    \end{align*}
    Salida múltiple: 
    \begin{align*}
        \rrnn 
            \longrightarrow 
        \rrnnmc
    \end{align*}
    Los números reales son una entelequia para los ordenadores. 
    \begin{align*}
       \mathcal{H}(\Q^d, \Q^s)
            \Rightarrow
        \rrnnmc
    \end{align*}
\end{frame}

% Para las citas que no estén mencionadas 
\begin{frame}
    \frametitle{Citas no mencionadas en ninún sitio}

    Este cándida diapositiva está solo para que pueda citar la 
    bibliografía y aparezca en la presentación. 
    \cite{MostafaLearningFromData}
    \cite{the-elements-of-real-analysis}

\end{frame}



% Blocks frame
\section{Blocks in Beamer}
\begin{frame}{Blocks in Beamer}
    \begin{block}{Standard Block}
        This is a standard block.
    \end{block}
    \begin{alertblock}{Alert Message}
        This block presents alert message.
    \end{alertblock}
    \begin{exampleblock}{An example of typesetting tool}
        Example: MS Word, \LaTeX{}
    \end{exampleblock}
\end{frame}
\end{document}