%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Alternativas al algoritmo de gradiente descendente 
%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Otras alternativas al alternativo de gradiente descendente}  

\textcolor{red}{ Apartado aun en borrador} 

Una de nuestras hipótesis era no imponernos 
hipótesis y nótese que en algoritmo de gradiente descendente se introduce la restricción de que 
las funciones de activación deben de ser diferenciales. 

No es de extrañar que si buscamos optimizaciones  se empiece a particularizar los componentes del problema, sin embargo ¿exigir estas restricciones está sustentando teóricamente? 

Es totalmente lícito y natural plantearse si otras alternativas al algoritmo de gradiente descendente podrían reducir el costo computacional del proceso de aprendizaje. 

Para ello se ha consultado el estado del arte actual  encontrando publicaciones como: 

\textcolor{red}{¿hay algo en optimización que merezca la pena?}

\cite{TransactionsOnNeuralNetworks}

