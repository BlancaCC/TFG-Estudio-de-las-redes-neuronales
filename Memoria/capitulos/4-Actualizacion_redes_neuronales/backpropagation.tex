%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Backpropagation 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%% algoritmo de gradiente descendente 

\section{El gradiente descendente} \label{sec:gradiente-descendente}

De acorde a los capítulos uno y dos del libro  \cite{learning-from-data-1-2},
 una vez concretado el problema y sus elementos 
(\ref{sub:componentes_aprendizaje}) es necesario definir un método con 
el que aproximar la función ideal $f,$ para ello introduciremos el algoritmo de gradiente descendente.  

El gradiente descendente es un método iterativo de minimización de funciones diferenciables. 

En nuestro caso particular se quiere aproximar la función ideal desconocida $f$ a partir de funciones (redes neuronales) $h_w \in \mathcal{H}$, donde $w \in M(\R)_{m \times n}$ son parámetros que caracterizan a $h_w$.
Para  
Dada también una función de error diferenciable y que no presente puntos de inflexión
$E: M(\R)_{m \times n} \longrightarrow \R,$
se toma una matriz inicial  cualquiera $w_0 \in M(\R)_{m \times n}$ y 
fijado $\eta \in \R^+$. 

Se define la sucesión 
\begin{equation}
    w_{t+1}  = w_t - \eta \nabla E(w_t).
\end{equation}  

Donde $w_n$ es una sucesión cuyos términos convergen a un mínimo local.
\subsubsection*{Observaciones sobre el algoritmo }

\begin{itemize}
    \item El algoritmo solo encuentra óptimos locales con una dependencia crucial del valor de inicio. 
    \item La convergencia no es segura en un tiempo finito y requiere de criterios de parada. 
    \item Si la función es convexa el mínimo será global.
    \item El parámetro $\eta$ puede ser cualquiera y debe de ser fijado o controlado por el diseñador.  
\end{itemize}




%%%%%%%%%% backpropagation 
\section{\textit{Backpropagation}}

Los parámetros que determinan una red neuronal son sus pesos, y esto es lo que \textit{aprende},
para actualizarlos utilizaremos la técnica de gradiente descendente ya explicado en la sección \ref{sec:gradiente-descendente}. 

\begin{equation}
    W(t+1) = w(t) - \eta \nabla E_{in}(w(t)). 
\end{equation}

Además, con el fin de reducir el coste del cálculo del gradiente, 
se utiliza el algoritmo conocido como \textit{backpropagation} que fue publicado en 
1989 en el artículo \cite{backpropagation-Hinton}. 

Sea $E_{in}(w)$ la función de error, la cual tomaremos como el error dentro de conjunto de entrenamiento, esto es,  si el conjunto 
de entrenamiento está constituido por $N$ datos de la forma $(x_n, y_n)$ con $x_n$ el vector de entrada y $y_n$ el estado o valor deseado para cualquier $n\in \{1, \ldots, N\}.$
\begin{equation}
    E_{in}(w) = \frac{1}{N} \sum^N_{n=1} (h_w(x)- y_n)^2. 
\end{equation}
Denotaremos como $e_n$ a 
\begin{equation}
    e_n = (h_w(x)- y_n)^2 
\end{equation}
que es una métrica para medir error entre, en nuestro caso  
la red neuronal $h_w$ y los valores deseados, con $w$ el vector que contiene las respectivas matrices de pesos de cada capa 
$W^{(l)} l \in \{1, \ldots, L\}.$  

%% Ejemplo 
Mostraremos un ejemplo primero antes de presentar el método general para facilitar la comprensión del algoritmo. 
% Imagen red neuronal simple
\begin{figure}[h!]
    \includegraphics[width=\textwidth]{introduccion_redes_neuronales/construccion_redes_neuronales/rrnn-1-2-1.drawio.png}
    \caption{Red neuronal con una capa oculta a la que se quiere aplicar \text{backpropagation}}
    \label{img:construccion_rrnn:rrnn-1-2-1}
\end{figure} 
Queremos actualizar los pesos $w$ de la red neuronal 
$f_w : \R \longrightarrow \R$ presentada en \ref{img:construccion_rrnn:rrnn-1-2-1}.
$f_w$ está compuesta de dos capas ocultas. Supongamos que nos basaremos en un dato 
$(x, y)$ así pues podemos suponer que 
\begin{equation}
    E_{in}(w) = \frac{1}{N}e(f_w(x), y) = \frac{1}{N} (f_w(x)- y)^2.
\end{equation}
Como queremos actualizar los pesos utilizando el método de gradiente descendente necesitamos calcular el gradiente $\nabla E_{in}(w)$, en nuestro caso tenemos $w=\{W^{(1)}, W^{(2)}\}$ con 
\begin{align}
    W^{(1)} = 
    \begin{bmatrix}
        w^{(1)}_{01} & w^{(1)}_{11} \\
        w^{(1)}_{02} & w^{(1)}_{12} \\
    \end{bmatrix} 
    \text{ y }
    W^{(2)} = 
    \begin{bmatrix}
        w^{(2)}_{01} & w^{(2)}_{11} & w^{(2)}_{21}\\
    \end{bmatrix}, 
\end{align}
luego 
\begin{equation}
    \nabla E_{in}(w) = 
    \left(
        % primera capa 
        \frac{\partial e}{\partial w^{(1)}_{01}},
        \frac{\partial e}{\partial w^{(1)}_{11}},
        \frac{\partial e}{\partial w^{(1)}_{02}},
        \frac{\partial e}{\partial w^{(1)}_{12}},
        % segunda capa
        \frac{\partial e}{\partial w^{(2)}_{01}},
        \frac{\partial e}{\partial w^{(2)}_{11}},
        \frac{\partial e}{\partial w^{(2)}_{21}}
    \right).
\end{equation} 
Cada parcial se calcula, utilizando la regla de la cadena como
\begin{align}
    \frac{\partial e}{\partial w^{(1)}_{01}} 
    &=
    \frac{\partial e}{\partial s_1^{2}}
    \frac{\partial s_1^{2}}{\partial w^{(1)}_{01}} 
    \\
    &= 
    \frac{\partial }{\partial w^{(1)}_{01}}
         \tanh \left(s^{(2)}_{1}\right)
    \\
    &= 
    \left(1- \tanh^2 \left(s^{(2)}_{1}\right)\right) 
    \frac{\partial s^{(1)}_{1}}{\partial w^{(1)}_{01}}
    \\
    &= 
    \left(1- \tanh^2 \left(s^{(2)}_{1}\right)\right) 
    \frac{\partial }{\partial w^{(1)}_{01}}
    \left(w^{(2)}x^{(1)}\right)
    \\
    &= 
    \left(1- \tanh^2 \left(s^{(2)}_{1}\right)\right) 
    \frac{\partial }{\partial w^{(1)}_{01}}
    \left(
        \sum^2_{i=0}
        w^{(2)}_{i1}x^{(1)}_i
    \right)
    \\
    &= 
    \left(1- \tanh^2 \left(s^{(2)}_{1}\right)\right) 
    \left(
        \sum^2_{i=0}
        w^{(2)}_{i1}\frac{\partial x^{(1)}_i }{\partial w^{(1)}_{01}}
    \right)
    \\
    &= 
    \left(1- \tanh^2 \left(s^{(2)}_{1}\right)\right) 
    \left(
        \sum^2_{i=1}
        w^{(2)}_{i1}\frac{\partial }{\partial w^{(1)}_{01}}
        \left(
            \tanh \left(s^{(1)}_{i}\right)
        \right)
    \right)
    \\
    &= 
    \left(1- \tanh^2 \left(s^{(2)}_{1}\right)\right) 
    \left(
        \sum^2_{i=1}
        w^{(2)}_{i1}
        \left(
            \left(1- \tanh^2 \left(s^{(1)}_{i}\right)\right)
            \frac{\partial  }{\partial w^{(1)}_{01}}
            \left(
                \sum^1_{j=0}\sum^2_{k=1}
                w^{(1)}_{j k}x^{(0)}_j
            \right)
        \right)
    \right)
    \\
    &= 
    \left(1- \tanh^2 \left(s^{(2)}_{1}\right)\right) 
    \left(
        \sum^2_{i=1}
        w^{(2)}_{i1}
        \left(
            \left(1- \tanh^2 \left(s^{(1)}_{i}\right)\right)
            x^{(0)}_0
        \right)
    \right).
\end{align}
Notemos que no se han evaluado las apariciones de $s_i^{(j)}$.
Otro ejemplo sería
\begin{align}
    \frac{\partial e}{\partial w^{(2)}_{21}} 
    &=
    \frac{\partial }{\partial w^{(2)}_{21}}
         \tanh \left(s^{(2)}_{1}\right)
    \\
    &= 
    \frac{\partial }{\partial w^{(2)}_{21}}
         \tanh \left(w^{(2)}x^{(1)}\right)
    \\
    &= \left(
    1- \tanh^2 \left(s^{(2)}_{1}\right) \right)x^{(1)}_2.
\end{align}

Notemos que no se han desarrolla los términos de la forma $s^{(i)}_j$. Además si existen $Q$ pesos la complejidad del cálculo será $\mathcal{O}(Q^2)$, sin embargo, como hemos visto existen términos que se repiten en ambas ecuaciones, por lo que utilizar una técnica de programación dinámica, concretamente la conocida como 
\textit{backpropagation} (\cite{backpropagation-Hinton}) nos permite reducir el coste a una complejidad de  $\mathcal{O}(Q).$

%% Fin del ejemplo 
%% COMIENZA EL ARTÍCULO 

%Formalizaremos primero qué parámetros debemos estimar del gradiente.

Sea $\theta$ una función de activación derivable, 
 $L$ el número de capas ocultas, $N$ el tamaño del conjunto de entrenamiento $x^{(0)} = (1, x_1, \ldots, x_{d^{(0)}})^T$ 
siendo $x = (x_1, \ldots, x_{d^{(0)}})^T$ la entrada de la red neuronal, $x^{(L)}$ la salida de la red neuronal y 
$d^{(l)}$ la dimensión, número de nodos en la capa $l$-ésima. 
Recordemos que 
para cualquier $l \in \{1, \ldots, L\}$ con
\begin{equation}
    x^{(l)}
     = 
     \theta \left( s^{(l)}\right) 
     = 
     \theta \left( W^{(l)} x^{(l-1)}\right),
\end{equation}
y
\begin{equation}
    E(w) = \frac{1}{N} 
    \sum_{n = 1}^{N}
    \sum_{i = 1}^{d^{(L)}}
    \left({x_n}^{(L)}_i-y_{n_i} \right)^2 
    = 
    \frac{1}{N}
    \sum^{N}_{n=1} e_n.
\end{equation}

%% Gradiente de la salida: 
Para simplificar la notación nos referiremos como $e$ a $e_n.$
Vamos a proceder a calcular primero los gradientes de la última capa, 
sea $w^{(L)}_{i j}$  con $j \in \{1, \ldots , d^{(L)}\}$, 
$i \in \{1, \ldots , d^{(L-1)}\}$  el peso que relaciona la salida 
$x_i ^{(L-1)}$ del  
nodo $i$ de la capa anterior con la entrada $s_j ^{(L)}$ del nodo $j$ de la última capa. 
Vamos a calcular $\frac{\partial e}{ w^{(L)}_{i j}}$ utilizando la regla de la cadena. 
\begin{equation}
    \frac{\partial{e}}{\partial w^{(L)}_{i j}}
     = 
     \frac{\partial{e}}{\partial x^{(L)}_j} 
     \frac{\partial x^{(L)}_j}{\partial s^{(L)}_j} 
     \frac{\partial s^{(L)}_j}{\partial w^{(L)}_{i j}}.
\end{equation}
Donde es fácil ver que para el tercer término
\begin{equation}\label{eq:backpropagation_s_última_capa_derivada}
    \frac{\partial s^{(L)}_{j}}{\partial w^{(L)}_{i j}}
    = 
    \frac{\partial }{\partial w^{(L)}_{i j}}
    \left(
        w^{(L)}_{\ast j } \cdot x^{(L-1)}
    \right)
    = 
    x^{(L-1)}_j
\end{equation}
Donde $w^{(L)}_{\ast j} = \left(w^{(L)}_{0 j}, w^{(L)}_{2 j}, \ldots, w^{(L)}_{d^{(L-1)} j}\right)$ representa los pesos correspondientes al nodo $j$,
 en forma de  vector fila y $x^{(L-1)} = \left(1, x ^{(L-1)}_1, \ldots, x ^{(L-1)}_{d^{(l-1)}}\right)^T$ el valor de la salida de la capa $L-1$
 en forma de vector columna.
 
 Por otro lado 
 \begin{equation}\label{eq:backpropagation_E_última_capa_derivada}
    \frac{\partial{e}}{\partial x^{(L)}_j} =
    2 
    \left(
    x^{(L)}_j - y_j
    \right)
 \end{equation}
 donde conocemos $x^{(L)}_j$ gracias al algoritmo de \textit{forward propagation}
 y $y_j$ la componente $j$-ésima del vector deseado en el entrenamiento.
y finalmente
\begin{equation}\label{eq:backpropagation_x_última_capa_derivada}
    \frac{\partial x^{(L)}_j}{\partial s^{(L)}_j} 
    = 
    \frac{d}{d s^{(L)}_j} 
        \theta \left( 
            s^{(L)}_j
        \right)
\end{equation}
que sabemos que se puede calcular por ser $\theta$ derivable y 
$s^{(L)}_j$ un valor conocido que ya ha sido calculado por el algoritmo de 
\textit{forward propagation.}

Por lo tanto, concluimos por 
(\refeq{eq:backpropagation_E_última_capa_derivada}),
(\refeq{eq:backpropagation_x_última_capa_derivada})
y  
(\refeq{eq:backpropagation_s_última_capa_derivada})
\begin{equation}
    \frac{\partial{e}}{\partial w^{(L)}_{i j}}
     = 
     \frac{\partial{e}}{\partial x^{(L)}_j} 
     \frac{\partial x^{(L)}_j}{\partial s^{(L)}_j} 
     \frac{\partial s^{(L)}_j}{\partial w^{(L)}_{i j}} 
    =
    2\left( x^{(L)}_j - y_j \right) 
    \theta' \left( s^{(L)}_j\right)
    x^{(L)}_j.
\end{equation}
%%%% Gradiente interior 
Denotaremos por \textit{sensibilidad} a 
\begin{equation}
    \delta^{(l)} = \frac{\partial e}{ \partial s^{(l)}}.
\end{equation}

Para calcular la derivada de pesos de capas interiores 
$l \in \{1 \ldots L-1\}$
procederemos de la siguiente manera, para 
$j \in \{1, \ldots , d^{(l)}\}$ y 
$i \in \{1, \ldots , d^{(l-1)}\}$ 
\begin{equation}
    \frac{\partial{e}}{\partial w^{(l)}_{i j}}
     = 
     \frac{\partial e}{\partial s^{(l)}_j} 
     \frac{\partial s^{(l)}_j}{\partial w^{(l)}_{i j}}
    = 
    \delta^{(l)}
    \frac{\partial}{\partial w^{(l)}_{i j}}
    w^{(l) \cdot x^{(l-1)}}
    = 
    \delta^{(l)} x^{(l-1)}_i,
\end{equation}
donde  $x^{(l-1)}_i$ es conocida por el algoritmo de 
$\textit{forward propagation}$. 

Por otra parte 
$\delta^{(l)}_j$ 
cumple que 
\begin{align}
    \delta^{(l)}_j 
    &= 
    \frac{\partial e}{\partial s^{(l)}_j}
    \\
    &= 
        \frac{\partial e}{\partial s^{(l+1)}}
        \frac{\partial s^{(l+1)}}{\partial s^{(l)}_j}
    \\
    &= 
    \delta^{(l+1)} 
    \frac{\partial}{\partial s^{(l)}_j}
        \left( w^{(l)} \cdot \theta(s^{(l)})\right)
    \\
    &= 
    \delta^{(l+1)}_j 
    w^{(l)}_j \cdot  \theta'(s^{(l)}_j). 
\end{align}
Por lo que de forma matricial tenemos que $\delta^{(l)}$ 
cumple que 
\begin{align}
    \delta^{(l)} 
    &= 
    \frac{\partial e}{\partial s^{(l)}}
    \\
    &= 
        \frac{\partial e}{\partial s^{(l+1)}}
        \frac{\partial s^{(l+1)}}{\partial s^{(l)}}
    \\
    &= 
    \delta^{(l+1)} 
    \otimes 
    \frac{\partial}{\partial s^{(l)}}
        \left( w^{(l)} \cdot \theta(s^{(l)})\right)
    \\
    &= 
    \delta^{(l+1)} 
    \otimes 
    \left(
    w^{(l)} \cdot \theta'(s^{(l)})
    \right). 
\end{align}

De esta manera a partir de las \textit{sensibilidades} de las 
capas posteriores es posible calcular las $l$-ésimas y puesto que 
la sensibilidad $\delta_{(L)}$ es conocida acabamos de determinar 
cómo calcular la derivada de todos los pesos  de manera constructiva. 
Procedemos a explicitar los cálculos. 

\subsubsection{Algoritmos para la actualización de los pesos}  

El razonamiento expuesto conduce al siguiente proceso algorítmico 
para el cálculo de los gradientes. 

% pseudo código cálculo de sensibilidades 
\begin{algorithm}[H]
    \caption{Algoritmo \textit{backpropagation} para calcular
    las sensibilidades $\delta^{(l)}$}
    \hspace*{\algorithmicindent} \textbf{Input}: un par de $(x,y)$ del conjunto de entrenamiento.  \\
    \hspace*{\algorithmicindent} \textbf{Output} 
    \begin{algorithmic}[1]
        % Forward propagation
        \STATE Se ejecuta el algoritmo de \textit{forward propagation} 
        
        con $x$ como entrada para calcular y guardar : 
        \begin{align}
            s^{(l)} \quad &\text{for } l = 1, \ldots, L;
            \\
            x^{(l)} \quad &\text{for } 0 = 1, \ldots, L;
        \end{align}
        % Inicializamos
        \STATE \COMMENT{Inicializamos sensibilidades últimas capas}
        \begin{equation}
            \delta^{(L)} \longleftarrow 2
            \left( 
                x^{(L)} - y
            \right)
            \theta' \left( s^{(L)} \right)
        \end{equation}
        \STATE 
        \COMMENT{ \textit{Backpropagation}}
        
        \For{$l = L-1$ to $1$}
        {
            \begin{equation}
                \delta^{(l)} 
                    \leftarrow
                \theta' 
                \left(
                    s^{(l)}
                \right)
                \otimes
                \left[
                    W^{(l+1)}
                    \delta^{(l+1)}
                \right]^{d^{(l)}}_1
            \end{equation}
        }
\end{algorithmic}
\end{algorithm}

% pseudo código cálculo cálculo de gradiente  
\begin{algorithm}[H]
    \caption{Algoritmo para el cálculo $E_{in}(w)$
    y $g = \nabla E_{in}(w).$}
    \hspace*{\algorithmicindent} \textbf{Input}:
    $w = \left\{ W^{(1)}, \ldots, W^{(L)}\right\};$
    $\mathcal{D} = (x_1, y_1), \ldots, (x_n, y_n).$\\
    \hspace*{\algorithmicindent} \textbf{Output:} error $E_{in}(w)$ y gradiente
    $g = \{G^{(1)}, \ldots, L\}.$  
    \begin{algorithmic}[1]
        \STATE Inicializamos $E_{in}(w)=0$ y $G^{(l)} = 0$ 
        for $l = 1, \ldots, L.$
        \STATE \For{ cada punto $(x_n, y_n),$
        n = 1, \ldots, N}{
            % cálculos de las x^l y sensibilidades
            \STATE Calcula $x^{(l)} \quad \text{for } l = 1, \ldots, L;$
            \COMMENT{ \textit{forward propagation}}
            \STATE Calcula $\delta^{(l)} \quad \text{for } l = L, \ldots, 1;$
            \COMMENT{ \textit{backpropagation}}
            % vamos sumando el erro en cada punto 
            \STATE
            \begin{equation}
                E_{in} 
                \leftarrow
                E_{in} + 
                \frac{1}{N}(x^{L}-y_n)^2 
            \end{equation}
            \For{l=1, \ldots, L}{
                \STATE
                \COMMENT{Calculamos gradiente en un punto}
                \begin{equation}
                    G^{(l)}\left( x_n\right) = 
                    \left[
                        x^{(l-1)}
                        \left(
                            \delta^{(l)}
                        \right)^T
                    \right]
                \end{equation}
                \STATE 
                \begin{equation}
                    G^{(l)}
                    \leftarrow
                    G^{(l)}
                    + 
                    \frac{1}{N}
                    G^{(l)}\left( x_n\right)
                \end{equation}
            }
        }
\end{algorithmic}
\end{algorithm}

Finalmente para conocer el valor actualizado de los pesos, lo actualizaremos usando la técnica de gradiente descendente. 

\begin{algorithm}[H]
    \caption{Algoritmo gradiente descendente.}
    \hspace*{\algorithmicindent} \textbf{Input}: Pesos $w$ y gradiente $g$. \\
    \hspace*{\algorithmicindent} \textbf{Output:} $w$ actualizado
    \begin{algorithmic}[1]
        \STATE
        \For{$l = 1, \ldots, L.$}{
            \begin{equation}
                W^{(l)}
                \leftarrow
                W^{(l)} 
                - 
                \eta G^{(l)}                    
            \end{equation}
        }
\end{algorithmic}
\end{algorithm}
