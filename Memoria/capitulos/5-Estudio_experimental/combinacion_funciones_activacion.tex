%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%    Combinación de distintas funciones 
%              de activación 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Futuros trabajos: Selección genética de las funciones de activación }
\label{ch08:genetic-selection}

Como se indicó en \ref{ch03:funcionamiento-intuitivo-funcion-activacion}
aunque la convergencia universal no 
dependa de la función de activación seleccionada,
fijado cierto número de neuronas, éstas sí que pueden 
determinar el error mínimo que podamos alcanzar.  

Gracias al resultado \ref{cor:se-generaliza-G-a-una-familia} es posible combinar en una red neuronal distintas funciones de activación y que el teorema de convergencia universal \ref{teo:MFNAUA} se mantenga cierto, esto abre la puerta a explorar también durante el entrenamiento diferentes funciones de activación. De hecho ya existen artículos como \cite{FunctionOptimizationwithGeneticAlgorithms} y \cite{Genetic-deep-neural-networks} donde se desarrolla de manera experimental esta idea. 

El problema que se tiene es que al aumentar
el número de funciones de activación candidatas, se está aumentando también el espacio de búsqueda; lo que significa que la complejidad del espacio aumenta y por ende el coste para encontrar una solución. 

Se ha intentado paliar la situación con algoritmos genéticos (véanse los artículos recién citados). Sin embargo, existen dos detalles claves y novedosos que podemos aportar: el primero es que \textbf{con nuestro teorema \ref{teo:eficacia-funciones-activation}}  se ha obtenido un 
criterio de selección de las funciones de activación que 
tendrán el mismo potencial de aproximación y menor coste; 
esto \textbf{nos ahorraría tener que explorar combinaciones de 
funciones de activación que no vaya a aportar en precisión y 
además aumenten el costo.} 


El segundo reside en que en los artículos que versan sobre el tema, 
utilizan modelos de \textit{deep learning} sensibles a la posiciones de las función de activación. Es decir, que para $n$ neuronas y $t$ funciones de activación diferentes el tamaño del espacio de búsqueda es $t^n$. Sin embargo, una de las ventajas que presenta \textbf{nuestro modelo} es que \textbf{es invariante ante cambios de posición de funciones de activación;} por lo que una vez fijado el número de cada tipo de funciones de activación da igual la neurona dónde se posicionen (esto es fácil de comprobar observando el modelo \ref{chapter:construir-redes-neuronales} y por la propiedad conmutativa de la suma).

Es decir, \textbf{con nuestro modelo y resultados se estaría reduciendo el espacio de búsqueda y por tanto merecería la pena plantearse de nuevo este tipo de experimentos}. 

