% !TeX root = ../../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Hipótesis planteadas 
%*******************************************************

\chapter{Hipótesis de optimización }
\textcolor{red}{ATENCIÓN: Todos este capítulo está como notas personales}

\section{A qué nos referimos con optimización}
ES necesario decir qué queremos optimizar
Ejemplo: 
- Mejores resultados para mismo tiempo. 

Medir error y tiempo de cálculo. 

Es por ello que es necesario establecer cómo lo vamos a medir.



\textcolor{red}{ATENCIÓN: Todo este capítulo está como notas personales}  


En esta sección recopilaremos las posibles ideas que podrían optimizar las 
redes neuronales, describiremos una experimentación para contrastar los resultados y mostraremos sus conclusiones. 

\section{Democratización de la función de activación}\label{hypothesis:activation-function}

La primera pregunta, existe alguna función de activación 
claramente mejor en algún sentido que las otras. 

Haciendo un estudio computacional de evaluaciones concretas sí. 
(TODO: hacer experimento)

Pero eso no significaría que fuera mejor para
evaluar los resultados en una red neuronal real. 
(hacer experimento)

Este experimento depende de los datos y da lugar a la siguiente pregunta. 

¿Existe una dependencia en la mejora de los resultados 
con respecto de los datos?

Es decir si tenemos dos redes neuronales $f$ y $g$  de mismo número de neuronas y distintas funciones de activación y dos conjuntos de entrenamiento $D_1$, $D_2$

¿Podría darse el caso de que para $D_1$ $f$ aprenda mejor pero que para $D_2$ $g$ sea mejor?. 


Vamos a tratar de encontrar de encontrar un ejemplo de esto.

\section{Inicialización de la pesos red neuronal}\label{hypothesis:pesos-iniciales}

\section{Construcción dinámica del número de neuronas}

