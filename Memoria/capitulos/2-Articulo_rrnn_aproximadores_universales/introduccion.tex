% !TeX root = ../../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Introducción artículo MFNAUA
%*******************************************************
\section{Las redes neuronales son aproximadores universales}  

Tras las definición \ref{sec:redes-neuronales-intro-una-capa} de red neural expuesta,
es pertinente la pregunta si tal estructura será 
capaz de aproximar con éxito una función genérica desconocida.   

Aunque las redes neuronales multicapa ya se venían aplicando con anterioridad, 
véase por ejemplo los usos expuestos durante la primera conferencia
internacional de redes neuronales de \cite{4307059} de 1987, 
no fue hasta 1989 que se descubrió formalmente su alcance.
 Tal delimitación se propuso en el artículo 
\textbf{Multilayer Feedforward Networks are Universal Approximators} \cite{HORNIK1989359}
 escrito por Kurt Hornik, Maxwell Stinchcombe y Halber White enunciando: 

\begin{teorema}\textbf{Las redes \textit{feedforward} multicapa son una clase de aproximadores universales } \label{teo:MFNAUA}
    \\
    Una red neuronal \textit{feedforward} multicapa estándar con tan solo una capa oculta y con una función de activación cualquiera es capaz de aproximar cualquier 
    función Borel medible  con dominios y codominios de dimensión finita (no necesariamente iguales) y con el nivel de precisión que se desee siempre y cuando 
    se utilicen suficientes neuronas. En este sentido las redes \textit{feedforward} multicapa son una clase de aproximadores universales.

\end{teorema}

En las secciones siguientes, con el fin de alcanzar una
 comprensión profunda de las redes neuronales,
trataremos de desgranar y profundizar en el artículo y su 
demostración. Primero precisaremos o introduciremos conceptos elementales 
sobre redes neuronales \ref{ch:articulo:sec:defincionesPrimeras}, después 
demostraremos el teorema en el caso real 
\ref{teo:TeoremaConvergenciaRealEnCompactosDefinicionesEsenciales} e iremos refinando y generalizando los resultados hasta probar
el resultado enunciado \ref{teo:MFNAUA} para una capa oculta.

El esquema general será: 

\begin{align*}
    \rrnn 
        \xRightarrow[]{\ref{teo:2_4_rrnn_densas_M}}  
    \rrnng 
        \xRightarrow[]{\ref{teorema:2_3_uniformemente_denso_compactos}}
    \pmcg
        \xRightarrow[]{\ref{teo:TeoremaConvergenciaRealEnCompactosDefinicionesEsenciales}}     
    \fC    
        \xRightarrow[]{\ref{teo:2_2_denso_función_continua}} 
    \fM.
\end{align*}

    % Nota margen de denso
    \setlength{\marginparwidth}{\bigMarginSize}
    \marginpar{\maginLetterSize
        \iconoAclaraciones \textcolor{dark_green}{ 
            \textbf{Idea intuitiva conjunto denso.}
        }
        Si $S$ es denso en $T$, 
        se está está diciendo que \textbf{los elementos de $S$ son capaces de aproximar cualquier elemento de $T$
        con la precisión que se desee}. 
    }


\begin{itemize}
    \item Las redes neuronales que nosotros hemos modelizado son densas en un espacio más general que hemos denominado \textit{Anillo de aproximación de redes neuronales}
    generado a partir de una función de activación $\psi$. 
    \item Que a su vez es denso en el \textit{Anillo de aproximación de redes neuronales}
    generado a partir de una función medible $G$. 
    \item El espacio \textit{Anillo de aproximación de redes neuronales} es denso en la funciones continuas.
    \item Las funciones continuas son densas en el espacio de funciones medibles. 
\end{itemize}

Si quisiéramos situar en este esquema a otras definiciones de redes neuronales las situaríamos entre  nuestro modelo y el espacio \textit{Anillo de aproximación de redes neuronales}; en  el capítulo \ref{chapter:construir-redes-neuronales} se probará tal resultado y analizarán los beneficios de basarnos en un modelo más simple. 


