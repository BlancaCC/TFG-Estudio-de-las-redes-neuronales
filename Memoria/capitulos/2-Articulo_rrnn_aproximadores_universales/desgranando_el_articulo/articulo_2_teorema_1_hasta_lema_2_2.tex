% !TeX root = ../../tfg.tex
% !TeX encoding = utf8
%
%*******************************************************
% Contenido del artículo 2: Primeros resultados
%*******************************************************


\section{Primeros resultados} 
% Introducción sección 


%%%%% primer teorema de convergencia  
% Teorema 2.1 
\begin{teorema} [Teorema de convergencia real en compactos]  \label{teo:TeoremaConvergenciaRealEnCompactosDefinicionesEsenciales}

    Sea G cualquier función continua no constante definida de $\R$ en $\R$. 
    Se tiene que $\pmcg$ es uniformemente denso para compactos en $\fC$.
\end{teorema}

% Significado del teorema 2.1
\marginpar{\maginLetterSize
    \iconoAclaraciones \textcolor{dark_green}{     
    \textbf{Qué se está probando en el teorema \ref{teo:TeoremaConvergenciaRealEnCompactosDefinicionesEsenciales}}
    }
    Podemos aproximar tanto como queramos cualquier función
continua con elementos de $\pmcg$, es decir a lo que llamamos  \textit{anillo de aproximación}. 
}

\begin{proof}
    Bastará probar que el conjunto $\pmcg$ satisface las hipótesis del teorema de
     Stone-Weierstrass \ref{ch:TeoremaStoneWeiertrass}.
    Lo primero será comprobar que $\pmcg$ es un álgebra, para ello veamos que:   

      % Claves de la demostración teorema 2.1
      \marginpar{\maginLetterSize\textbf{Clave de la demostración del teorema \ref{teo:TeoremaConvergenciaRealEnCompactosDefinicionesEsenciales}}
      Se comprueba primero que para operando elementos del \textit{anillo de aproximación} con sumas, productos y re-escalados no se \textit{salen del conjunto}, es decir que el resultado es otro elemento del \textit{anillo de aproximación}. Con estas operaciones se irá construyendo y refinando una función que aproxime cualquier función continua con un método cuya idea es \textit{con este elemento del anillo de aproximación tengo tal error porque me falla en tales puntos, si le sumo esto otro corrijo ese error y además el resultado sigue perteneciendo al conjunto.}
      }      
    \begin{enumerate}
        \item La función constante uno pertenece al conjunto. 
        Como $G$ no es constante existirá un valor de la imagen distinto de $0$, supongamos que $G(a)= b \neq 0$ para $a,b \in \R.$
        Consideremos la función afín $A(x) = 0 \cdot x + a$, está claro que $\frac{1}{b}G(A(x))$ es la función constantemente uno. 
        \item El conjunto $\pmcg$ es cerrado para sumas y producto por escalares reales. 
        En efecto, si $f,g$ pertenecen a  $\pmcg$, serán de la forma
         $f = \sum_{j = 1} ^q  \beta_{fj} \prod_{k=1}^{l_{fj}}  G(A_{fjk}(x))$ y 
        $g = \sum_{j = 1} ^p  \beta_{gj} \prod_{k=1}^{l_{gj}}G(A_{gjk}(x))$  por lo que
        \begin{equation}
            \begin{split}
                \gamma f+ \sigma g =& \gamma \sum_{j = 1} ^q  \beta_{fj} \prod_{k=1}^{l_{fj}}  G(A_{fjk}(x)) + 
                \sigma \sum_{j = 1} ^p  \beta_{gj} \ \prod_{k=1}^{l_{gj}}G(A_{gjk}(x)) \\
                & = \sum_{j = 1} ^q  (\gamma \beta_{fj})  \prod_{k=1}^{l_{fj}}  G(A_{fjk}(x)) + 
                \sum_{j = 1} ^p  (\sigma \beta_{gj}) \ \prod_{k=1}^{l_{gj}}G(A_{gjk}(x)).
            \end{split}
        \end{equation}
        
        Basta renumerar una de las sumatorias para ver $\gamma f+ \sigma g$ como una combinación 
        lineal de productos finitos y por tanto $\gamma f+ \sigma g \in \pmcg.$
        
      
        \item Cerrado para producto. Para $f,g \in \pmcg$, se tiene que $fg$ pertenece a $\pmcg$, para ello basta ver que: 
        \begin{align}
           f g & = 
           % escribimos f  en forma del anillo
           \left(
               \sum_{i \in I_f} \beta_{f_i} \prod_{k=1}^{l_i} G((x)) 
           \right) 
           % escribimos g en forma del anillo
           \left(
               \sum_{j \in I_g} \beta_{g_j} \prod_{k=1}^{l_j} G(A_{g_{j k}(x)}) 
           \right) 
           \\
           & = 
           % propiedad distributiva (con la primera suma) 
            \sum_{i \in I_f} 
            \left(
                \beta_{f_i} \prod_{k=1}^{l_i} G(A_{f_{i k}}(x)) 
           \right) 
           % escribimos g en forma del anillo
           \left(
               \sum_{j \in I_g} \beta_{g_j} \prod_{k=1}^{l_j} G(A_{g_{j k}(x)}) 
           \right) 
           \\
           & = 
           \sum_{
               \begin{subarray}{c}
                i \in I_f \\
                j \in I_g
               \end{subarray}
               }
               (\beta_{f_i} \beta_{g_j})
               \sum_{k = 1}^{l_i + l_j}G 
               \left(
                    \tilde{A}_{i j k}(x)
               \right).
        \end{align}
        Definimos $\tilde{A}_{i j k}(x) = A_{f_{i k}}$ si $k \in \{1, \ldots, l_i\}$
        y $\tilde{A}_{i j k}(x) = A_{g_{i (k - l_i)}}$ si $k \in \{l_i + 1, \ldots, l_i+l_j\}$.
        luego $fg \in \pmcg$. 
    \end{enumerate}

    Veamos que $\pmcg$ separa puntos para cada compacto $K \subset \R^r$. 

    Por ser $G$ no constante existirán $a,b \in \R$ distintos cumpliendo que $G(a) \neq G(b)$. Fijadas $x,y \in K$ tomamos entonces cualquiera de las 
    funciones afines que cumplen que $A(x) = a$ y $A(y)=b$ 
    \footnote{Sabemos que al menos una habrá, ya que podemos plantear la función afín
    como un sistema de ecuaciones lineales de $r+1$ incógnita y 2 soluciones}, 
    por lo que $G(A(x)) \neq G(A(y))$ y tenemos como buscábamos que $\pmcg$ separa los puntos de $K$. 

    Comprobemos finalmente que para todo punto de $K$ existe una función de $\pmcg$  en el que la imagen no es nula.  

    Por ser $G$ no constante volvemos a tomar un $a \in \R$ tal que $G(a) \neq 0$, consideramos ahora la aplicación lineal
    $A(x) = 0 \cdot x + a$ por lo que para todo $x \in K$, $G(A(x)) = G(a) \neq 0$. 

    Como hemos comprobado se verifican todas las hipótesis del teorema de Stone-Weierstrass, con lo que concluimos que $\pmcg |_K$ es denso en $C(K)$. 
\end{proof}

\subsection{Observaciones y reflexiones sobre el teorema de convergencia real en compactos}

Con esto lo que acabamos de probar que una estructura más general de\textit{feedforward neural networks} con tan solo una capa oculta  son capaces de aproximar cualquier 
función continua en un compacto.  Cabe destacar que a la función $G$, que haría el papel de función de activación,
 solo se le ha pedido como 
hipótesis ser continua.     

%%% Corolarios propios 

Notemos que la función de activación $G$ es única en toda la estructura,
sin embargo es habitual la combinación de éstas en una misma red neuronal (
\cite{DBLP:journals/corr/abs-1811-03378}, 
 \cite{8258768}, 
 \cite{DBLP:journals/corr/SzegedyVISW15}
). 
%Nota sobre experimentar con 
%\setlength{\marginparwidth}{\smallMarginSize}
\reversemarginpar
\marginpar{\maginLetterSize \iconoProfundizar \textcolor{blue}{\textbf{Nueva hipótesis de optimización}}
El corolario \ref{cor:se-generaliza-G-a-una-familia}
abre la puerta a preguntarse si la combinación de diferentes funciones de activación 
podría mejorar los resultados de alguna manera.
}
\normalmarginpar

\begin{aportacionOriginal}

\begin{corolario}[Pueden combinarse distintas funciones de activación en una misma red neuronal] \label{cor:se-generaliza-G-a-una-familia}

    Una misma red neuronal puede estar constituida por una familia de funciones continuas no constantes $\Gamma$, 
    bastará con generalizar $\pmcg$ a $\sum \prod ^d (\Gamma)$ donde 
    \begin{equation}
        \begin{split}
            \sum \prod^d (\Gamma) = \{ 
                &f: \R^d \longrightarrow \R /
                f(x) = \sum_{j = 1} ^q  \beta_j \prod_{k=1}^{l_j}
                G(A_{jk}(x)), \\
                &x  \in \R^d, \beta_j \in \R, A_{jk}\in \afines, l_j,q \in \N, G \in \Gamma
                )
                \}
        \end{split}
    \end{equation}
\end{corolario}
\begin{proof}
    La demostración es idéntica a la dada en el Teorema de convergencia 
    real en compactos \ref{teo:TeoremaConvergenciaRealEnCompactosDefinicionesEsenciales}.
\end{proof}
\end{aportacionOriginal}

Notemos que este resultado no da pista alguna de las ventajas de una función frente a otra,
 ni cómo afecta a la \textit{velocidad de convergencia}. 


Recordemos que de manera general se ha definido $A$ como una función afín 
$A(x) = w \cdot x + b$ donde $x$ y $w$ son vectores de $\R^d$  y $b \in \R$ es un escalar.  ¿Pero que ocurriría si trabajáramos con transformaciones más generales?  
Por ejemplo $B((x_1, ..., x_d)) = \sum_{i= 0} ^N \sum_{j= 0} ^d \alpha_{ij} x_j^i$  con $N$ natural positivo. 

% Nota sobre posible hipótesis de optimización 
\setlength{\marginparwidth}{\bigMarginSize}
\marginpar{\maginLetterSize \iconoProfundizar \textcolor{blue}{\textbf{Nueva hipótesis de optimización}}
Gracias al corolario \ref{corolario:generaliza-a}
nos podemos plantear si aumentar el espacio de búsqueda a partir de transformaciones
 no lineales es más conveniente que hacerlo mediante el número neuronas.
Abordaremos esta cuestión en \ref{hypothesis:activation-function}.
}

\begin{aportacionOriginal}
    \begin{corolario}[Generalización de de las transformaciones afines]  \label{corolario:generaliza-a}
        Se puede extender $\afines$ a conjuntos más generales como el de los polinomios de $d$ variables de grado $N$, $\mathbb{P}$.  
    \end{corolario}
    \begin{proof}
        Simplemente hay que reparar en que $\afines$ está contenido en el espacio $\mathbb{P}$. 
        Es más observando la demostración bastará con utilizar cualquier conjunto que contenga a $\afines$. 
    \end{proof}
\end{aportacionOriginal}

% Nota idea intuitiva  equivalencia 
\marginpar{\maginLetterSize\raggedright
    \iconoAclaraciones \textcolor{dark_green}{ \textbf{Idea intuitiva equivalencia de funciones:}}}
    \marginpar{\maginLetterSize
    En la definición \ref{definition:equivalencia_funciones}
Se considera que dos funciones son equivalentes si 
son iguales en casi todos sus puntos.  El objetivo de todas estas definiciones es \textbf{saber cuándo una red neuronal es equivalente a una función medible o cuánto la aproxima}.
}
%% Definiciones de equivalencia de funciones 
\begin{definicion}[Equivalencia entre funciones] \label{definition:equivalencia_funciones}
    Sea $\mu$ una medida de probabilidad en $(\R^d, B^d)$.  Dos funciones 
    $f$ y $g$ pertenecientes a $\fM$, diremos que son $\mu -$equivalentes 
    si $\mu\{ x \in \R^d : f(x)=g(x) \} = 1.$
\end{definicion}

Lo que se está diciendo es que serán iguales casi por doquier.   


% Definición distancia  
\begin{definicion} [Introducción de una distancia basada en una probabilidad] \label{definition:distancia-probabilidad}
    Dada una medida de probabilidad $\mu$ en $(\R^d, B^d)$, se define 
    la métrica $\rho_{\mu}$ definida como 
    \begin{equation}
        \begin{split}
            & \rho_{\mu} : \fM \times \fM \longrightarrow \R^+ \\
            & \rho_{\mu}(f,g) = \inf \{ \epsilon > 0: \mu \{ x : |f(x) - g(x)| > \epsilon \} < \epsilon \}.
        \end{split}
    \end{equation}
\end{definicion}  

Con esta definición lo que se está buscando es una forma de decir cuánto 
distan las funciones $f,g$ entre ellas.  


%% Lema 2.1
\begin{lema}[Caracterización de la convergencia de una sucesión]\label{lema:caracterizacionConvergenciaSucesiones2_1}
    Son equivalentes las siguientes afirmaciones: 
    \begin{enumerate}
        \item $\rho_{\mu}(f_n, f) \longrightarrow 0$.
        \item Para cualquier  $\epsilon > 0$ se tiene que $\mu \{  x : |f_n(x) - f(x)| > \epsilon \} \longrightarrow 0$.
        \item $\int \min \{ |f_n(x) - f(x)|, 1\} d\mu(x) \longrightarrow 0.$
    \end{enumerate}
\end{lema}

% Nota idea intuitiva distancia
\marginpar{\maginLetterSize\raggedright
    \iconoAclaraciones \textcolor{dark_green}{ \textbf{Idea intuitiva distancia probabilidad:}}}
\marginpar{\maginLetterSize
Este concepto de analizar la tendencia de la mayoría de los punto 
se ve reflejado en la definición de distancia basada en una probabilidad \ref{definition:distancia-probabilidad}, siendo entonces la \textbf{distancia de dos funciones la menor de las distancias que siguen la mayoría de sus puntos}.

El lema \ref{lema:caracterizacionConvergenciaSucesiones2_1} nos permitirá trabajar con mayor comodidad matemática este concepto.
}

\begin{proof}
    % 1 -> 2
    Comenzaremos probando (1) $\Rightarrow$ (2). 

    Si $\rho_{\mu}(f_n, f) \longrightarrow 0$
    Fijamos $\epsilon_0 > 0$, tenemos por definición que 
    para cualquier $0 < \delta < \epsilon_0$ existirá $n_0 \in \N$ tal que 
    $\rho_{\mu}(f_n, f) < \delta$ para cada $n$ un natural mayor que $n_0$. Es decir,  
    

    $$\inf \{ \epsilon > 0: \mu \{ x : |f_n(x) - f(x)| > \epsilon \} < \epsilon \} < \delta \quad \forall n \geq n_0$$

    entonces 

    \begin{equation}
        \mu \{ x : |f_n(x) - f(x)| > \epsilon_0 \}
        \leq
        \mu \{ x : |f_n(x) - f(x)| > \delta\}
        < \delta 
        \quad 
        \forall n \geq n_0
    \end{equation}

    lo que significa que 

    \begin{equation}
        \mu \{ x : |f_n(x) - f(x)| > \epsilon_0 \}
        \longrightarrow
        0  
    \end{equation}
    probando con ello la implicación buscada.

    % 2 -> 1
    Veamos ahora que (2) $\Rightarrow$ (1). 
    Fijamos $\epsilon_0 > 0$ y bajo la hipótesis segunda se tiene que 

    \begin{equation}
        \mu \{ x : |f_n(x) - f(x)| > \epsilon_0 \}
        \longrightarrow
        0,  
    \end{equation}
    es decir, que para cualquier real $\delta$ cumpliendo que $0 < \delta < \epsilon_0$ 
    existe un natural $n_0$ a partir del cual todo natural $n$ mayor o igual satisface que 
    
    \begin{equation}
        \mu \{ x : |f_n(x) - f(x)| > \epsilon_0 \}
        \leq
        \mu \{ x : |f_n(x) - f(x)| > \delta\}
        < \delta 
        \quad 
        \forall n \geq n_0
    \end{equation}

    lo que significa que 
    
    \begin{equation}
        \inf \{ \epsilon > 0:
         \mu \{ 
             x : |f_n(x) - f(x)| > \epsilon \} < \epsilon 
             \} 
        < \delta 
        \quad 
        \forall n \geq n_0
    \end{equation}

    que por definición de la distancia equivale a que 

    \begin{equation}
        \rho_{\mu}(f_n, f) < \delta \quad \forall n \geq n_0
    \end{equation}

    probando con ello 

    \begin{equation}
        \rho_{\mu}(f_n, f) \longrightarrow 0. 
    \end{equation}

    % 2 -> 3
    Probaremos ahora que (2) $\Longrightarrow$ (3).   

    Por (2) se tiene que sea cual sea el $\epsilon$ cumpliendo que 
    $0 < \epsilon \leq 2$ 
    existirá un natural $n_0$ a partir del cual, cualquier otro natural $n$ 
    satisface que 
    \begin{equation} 
        \mu \{  
            x : |f_n(x) - f(x)| > \frac{\epsilon}{2}  
            \}  
        < 
        \frac{\epsilon}{2},  
    \end{equation}

    Gracias a esta desigualdad, para cualquier $n > n_0$ podemos acotar la siguiente integral: 

    \begin{equation}
        \int \min \{ |f_n(x) - f(x)|, 1\} d\mu(x) 
        \leq
        \frac{\epsilon}{2} (1-\frac{\epsilon}{2}) + 1\frac{\epsilon}{2} 
         = \epsilon - \frac{\epsilon^2}{4} <  \epsilon.  
    \end{equation}
    probando con ello la implicación (2) $\Longrightarrow$ (3).

    % 3 -> 1
    Finalmente comprobaremos la implicación (3) $\Longrightarrow$ (1).

    Para cada $n\in \N$ llamamos $g_n = \min\{|f_n - f|, 1|\}$.
    Por (2), dado $0 < \epsilon < 1$, existe un $n_0 \in \N$
    de modo que si $n \geq n_0$ se cumple que 
    \begin{equation}\label{eq:definiciones_Básicas_Integral_GN_menor_Epsilon_Cuadrado}
        \int g_n d\mu < \epsilon^2
    \end{equation}
    Como $\epsilon < 1$ tenemos que 

    \begin{equation}
        \{ x; g_n(x) > \epsilon \}
         = 
         \{ x; |f_n - f| > \epsilon \}
    \end{equation}

    luego 

    \begin{equation}
        \mu\{ x; |f_n - f(x)| > \epsilon \}
        = 
        \mu\{ x; g_n(x) > \epsilon \}
        \leq
        \frac{1}{\epsilon} 
        \int_{g_n(x) > \epsilon} g_n d\mu 
        < \epsilon 
        \quad
        \forall n \geq n_0
    \end{equation}

    donde se ha usado la desigualdad de Chebyshev para $g_n$ y la desigualdad 
    (\refeq{eq:definiciones_Básicas_Integral_GN_menor_Epsilon_Cuadrado}). 

Probando con esto lo buscado que  para cualquier  $\epsilon > 0$ se tiene que 
$$\mu \{  x : |f_n(x) - f(x)| > \epsilon \} \longrightarrow 0.$$
\end{proof}


%% Lema 2.2
\begin{lema} \label{lema:2_2_convergencia_uniforme_en_compactos}  
    Si $\{f_n\}$ es una sucesión de funciones en $\fM$ que converge
    uniformemente en un compacto a $f$ entonces $\rho_{\mu}(f_n, f) \longrightarrow 0$. 
\end{lema}  
\begin{proof} Para cada $n\in \N$ llamamos $g_n = \min\{|f_n - f|, 1|\}$.
    Tengamos presente que por el  lema \ref{lema:caracterizacionConvergenciaSucesiones2_1} 
    deberemos probar que para cualquier $\epsilon > 0$, 
    existe un $n_0$ natural, tal que para cualquier otro natural $n$ mayor o igual que $n_0$ se tiene que 

    \begin{equation}
        \int \min \{ |f_n(x) - f(x)|, 1\} d\mu(x) 
        < 
        \frac{\epsilon}{2}.
    \end{equation}  

    Sea $\mu(\R^d) = M \in \R^+$  y 
    sin pérdida de generalidad puede suponerse $M = 1$
     \footnote{De otra forma bastaría con definir 
    en los pasos siguientes $\mu(K) > M - \frac{\epsilon}{2}$ y acotar con $\frac{\epsilon}{2M}$ 
    en vez de $\frac{\epsilon}{2}$.}. 
    Ya que $\R^r$ es un espacio métrico localmente compacto
    (pag 228 teorema 52.G \cite{nla.cat-vn1819421}),
    se tiene que existirá un subconjunto $K$ compacto de $\R^r$ con medida $\mu(K) > 1 - \frac{\epsilon}{2}.$
    Para el cual, por su compacidad, existirá un  $n_0$ natural 
    $\sup_{x \in K} |f_n(x) - f(x)| < \frac{\epsilon}{2}$   
    para cada natural $n$ con $n\geq n_0.$ 
    
          % Nota idea intuitiva  lema de que C es denso en M
          \marginpar{\maginLetterSize\raggedright
          \iconoAclaraciones \textcolor{dark_green}{ \textbf{Idea intuitiva lema \ref{lema:A_1_C_es_denso_en_M}:}}
          }
          \marginpar{\maginLetterSize
          Las funciones continuas pueden tomar formas muy variopintas, estando incluso no acotadas. La función de Dirichlet definida en $D(x) = 1$ si $x$ es irracional y $D(x)=0$ si $x$ es racional, es medible pero no es continua ya que presenta infinitas discontinuidades.
          }
          \marginpar{\maginLetterSize
          Sin embargo, las funciones continuas son más simples, fáciles de entender y manejar. 
          Gracias al lema \ref{lema:A_1_C_es_denso_en_M}
          acabamos de probar que \textbf{podemos aproximar en
          casi todos sus puntos  cualquier función medible a partir de una continua.} 
          }
          \marginpar{\maginLetterSize
          Por ejemplo la función constantemente uno aproxima a la función $D$ previamente definida.
          }
    De modo que para cualquier $x \in K$, 
     $n$ con $n\geq n_0$   se cumple que 
     \begin{equation}
        |f_n(x) - f(x)| 
        = 
        \min \{ |f_n(x) - f(x)|, 1\} 
        = 
        g_n.
     \end{equation}

    Por lo que  
    \begin{equation} \label{eq:lema3_2_integral_en_compacto_K}
        \int_K g_n d\mu 
        \leq
         \mu(K) \sup_{x \in K} |f_n(x) - f(x)| 
        \leq 
        \frac{\epsilon}{2} .
    \end{equation}

    Acotando el primer sumando por la medida 
    del complemento de la región integrada y en virtud de 
    (\refeq{eq:lema3_2_integral_en_compacto_K})

    \begin{equation}
        \begin{split}
            \int_{\R^d \setminus K} \min \{ |f_n(x) - f(x)|, 1\} d\mu(x) 
            +
            \int_{K} \min \{ |f_n(x) - f(x)|, 1\} d\mu(x)  \\ \leq
            \mu(\R^d \setminus K) +  \frac{\epsilon}{2}
            \leq
            \frac{\epsilon}{2} +  \frac{\epsilon}{2}
            = 
            \epsilon
        \end{split}
    \end{equation}

    para cualquier $n \geq n_0$. 
\end{proof}



% Lema A.1 
\begin{lema}\label{lema:A_1_C_es_denso_en_M}
    Para cualquier medida finita $\mu$ se tiene que $\fC$ es denso en 
    $\fM$ para la distancia $\rho_\mu$.
\end{lema}
\begin{proof}
    Dada cualquier $f \in \fM$ y un $\epsilon > 0$ arbitrario, 
    tenemos que encontrar una función $g$ que cumpla que 
    $\rho_{\mu}(f, g) < \epsilon$. 

    Tomando un $M > 1$ lo suficientemente grande, tenemos que 
    
    \begin{equation}
        \int \min \{ |f(x)\ 1_{|f(x)| < M} - f(x)|, 1\} d\mu(x)
        < \frac{\epsilon}{2}. 
    \end{equation}

    Sabemos además que podemos aproximar $f 1_{|f| < M}$ por $g$, una función continua que es límite de una sucesión de
    funciones simples ( pag 241-242,  teoremas 55C y 55D \cite{nla.cat-vn1819421}), 
    la cual satisface 
    \begin{equation}\label{eq:lema3_3_integral}
        \int \min \{ |f(x) 1_{|f(x)| < M} - g(x)|, 1\} d\mu(x) 
        < \frac{\epsilon}{2}. 
    \end{equation}
    Tomamos $M$ lo suficientemente grande, de tal forma que 
    \begin{equation} \label{eq:lema3_3_medida_conjunto}
        \mu(\{ x: |f(x)| \geq M\}) < \frac{\epsilon}{2}
    \end{equation}
    y denotamos por $\Lambda$ al conjunto $\{ x: |f(x)| < M\}.$
    
    Concluyendo por \refeq{eq:lema3_3_integral} y 
    \refeq{eq:lema3_3_medida_conjunto}
     \begin{equation}
        \begin{split}
            \int \min \{ |f  - g|, 1\} d\mu 
            = 
            \int_\Lambda \min \{ |f1_{|f(x)| < M}  - g|, 1\} d\mu
            + 
            \int_{\R^d \setminus \Lambda} \min \{ |f  - g|, 1\} d\mu 
            \\
            <
            \frac{\epsilon}{2} 
            + 
            \mu(\{ x: |f(x)| \geq M\}) 
            <
            \frac{\epsilon}{2} 
            + 
            \frac{\epsilon}{2} 
            < \epsilon. 
    \end{split}
    \end{equation}
\end{proof}








