`%----- Teoría de la aproximación -----
% Texto principal del que se ha sacado 
@book{the-elements-of-real-analysis,
  title={The elements of real analysis},
  author={Robert G. Bartle},
  year={1947},
  publisher={John. Wiley \& Sons}
}

% ---------- Introducción a las redes neuronales -------

% Importancia y estado del arte del aprendizaje automático en la actualidad
@article{importancia-arte-aprendizaje-automatico,
	abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has 
  a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social 
  media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and 
  automated applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is 
  the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of 
  machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a 
  comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the 
  capabilities of an application. Thus, this study's key contribution is explaining the principles of different 
  machine learning techniques and their applicability in various real-world application domains, such as 
  cybersecurity systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the
   challenges and potential research directions based on our study. Overall, this paper aims to serve as a 
   reference point for both academia and industry professionals as well as for decision-makers in various 
   real-world situations and application areas, particularly from the technical point of view.},
	author = {Sarker, Iqbal H. },
	doi = {10.1007/s42979-021-00592-x},
	id = {Sarker2021},
	isbn = {2661-8907},
	journal = {SN Computer Science},
	number = {3},
	pages = {160},
	read = {1},
	title = {Machine Learning: Algorithms, Real-World Applications and Research Directions},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s42979-021-00592-x},
	volume = {2},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1007/s42979-021-00592-x}
}

@book{learning-from-data-1-2,
  title={Learning From Data: Concepts, Theory, and Methods},
  author={Vladimir Cherkassky and Filip Mulier},
  year={2007},
  publisher={John. Wiley \& Sons},
  edition        = {2}, 
  chapter        = {1,2},
}

@online{e-chapter-7-neural-networks,
  author = { Yaser Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Li},
  title = {{Neural Networks}},
  year = {2015},
  urldate = {Jan-2015},
  chapter = {7},
}

% algoritmo backpropagation 
@article{backpropagation-Hinton,
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	da = {1986/10/01},
	date-added = {2022-02-22 07:39:52 +0100},
	date-modified = {2022-02-22 07:40:25 +0100},
	doi = {10.1038/323533a0},
	id = {Rumelhart1986},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6088},
	pages = {533--536},
	read = {0},
	title = {Learning representations by back-propagating errors},
	ty = {JOUR},
	url = {https://doi.org/10.1038/323533a0},
	volume = {323},
	year = {1986},
	Bdsk-Url-1 = {https://doi.org/10.1038/323533a0}
  }


% historia redes neuronales  
@online{hisour,
  author = {Hisour},
  title = {Nocción del aprendizaje automático},
  year = 2021,
  url = {https://www.hisour.com/es/machine-learning-42773/},
  urldate = {2021-11-27}
}

@online{samuel-wikipedia,
  author= {Wikipedia. Arthur Samuel},
  url = {https://en.wikipedia.org/wiki/Arthur_Samuel},
  urldate = {2021-11-30}
}
@book{tom-michell-machine-learning,
   title={Machine Learing},
   author={Tom Mitchell},
   year={1997},
   publisher={McGraw},
    url = {https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html},
}
@online{mitchell-wikipedia,
  author= {Wikipedia. Tom Mitchell},
  url = {https://en.wikipedia.org/wiki/Tom_M._Mitchell},
  urldate = {2021-11-30}
}


%%% ----- Construcción de las redes neuronales   --------
%% Manual muy extenso , lo has usado para
%% Definición perceptrón 
%% Feedforward networks functions: capítulo 5 páginas 227-256
@book{BishopPaterRecognition,
author = {Bishop, Christopher M.}, 
title = {Pattern Recognition and Machine Learning (Information Science and Statistics)}, 
year = {2006}, 
isbn = {0387310738}, 
publisher = {Springer-Verlag}, 
address = {Berlin, Heidelberg} 
}

%% Manual también muy interesante
%% Usado para los tipos de aprendizaje Capítulo 1
@book{MostafaLearningFromData,  
author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien}, 
title = {Learning From Data}, 
year = {2012}, 
isbn = {1600490069},
 publisher = {AMLBook}, 
 abstract = {Machine learning allows computational systems to adaptively 
 improve their performance with experience accumulated from the observed
data. Its techniques are widely applied in engineering, science, finance, and commerce.
This book is designed for a short course on machine learning. It is a short course, not 
a hurried course. From over a decade of teaching this material, we have distilled what
we believe to be the core topics that every student of the subject should know. 
We chose the title `learning from data' that faithfully describes what the subject 
is about, and made it a point to cover the topics in a story-like fashion. 
Our hope is that the reader can learn all the fundamentals of the subject by 
reading the book cover to cover. ---- Learning from data has distinct theoretical
and practical tracks. In this book, we balance the theoretical and the practical,
the mathematical and the heuristic. Our criterion for inclusion is relevance. 
Theory that establishes the conceptual framework for learning is included, 
and so are heuristics that impact the performance of real learning systems. 
---- Learning from data is a very dynamic field. Some of the hot techniques
and theories at times become just fads, and others gain traction and become
part of the field. What we have emphasized in this book are the necessary 
fundamentals that give any student of learning from data a solid foundation,
and enable him or her to venture out and explore further techniques and theories, 
or perhaps to contribute their own. ---- The authors are professors at California 
Institute of Technology (Caltech), Rensselaer Polytechnic Institute (RPI), 
and National Taiwan University (NTU), where this book is the main text for 
their popular courses on machine learning. The authors also consult extensively 
with financial and commercial companies on machine learning applications,
and have led winning teams in machine learning competitions.}          
}


%% Ejemplos de tipo de aprendizaje en los que se utiliza redes neuronales: 

% artículo donde en el estado del arte dice que se aplica en 
% todas los tipos de aprendizajes 
%y indica como mejorarlo en el no supervisado 

@INPROCEEDINGS{8612259,
  author={Dike, Happiness Ugochi and Zhou, Yimin and Deveerasetty, Kranthi Kumar and Wu, Qingtian},
  booktitle={2018 IEEE International Conference on Cyborg and Bionic Systems (CBS)}, 
  title={Unsupervised Learning Based On Artificial Neural Network: A Review}, 
  year={2018},
  volume={},
  number={},
  pages={322-327},
  doi={10.1109/CBS.2018.8612259}}

% para aprendizaje por refuerzo 
@article{DBLP:journals/corr/BakerGNR16,
  author    = {Bowen Baker and
               Otkrist Gupta and
               Nikhil Naik and
               Ramesh Raskar},
  title     = {Designing Neural Network Architectures using Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1611.02167},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.02167},
  eprinttype = {arXiv},
  eprint    = {1611.02167},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BakerGNR16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% Ejemplo que utiliza algoritmos evolutivos para configurar la
red neuronal 
@inproceedings{10.5555/2955491.2955578, 
author = {Stanley, Kenneth O. and Miikkulainen, Risto}, 
title = {Efficient Reinforcement Learning through Evolving Neural Network Topologies}, 
year = {2002}, 
isbn = {1558608788}, 
publisher = {Morgan Kaufmann Publishers Inc.}, 
address = {San Francisco, CA, USA}, 
abstract = {Neuroevolution is currently the strongest method on the pole-balancing benchmark reinforcement learning tasks. 
Although earlier studies suggested that there was an advantage in evolving the network topology as well as connection weights, the leading neuroevolution systems evolve fixed networks. 
Whether evolving structure can improve performance is an open question. In this article, we introduce such a system, NeuroEvolution of Augmenting Topologies (NEAT). We show that 
when structure is evolved (1) with a principled method of crossover, (2) by protecting structural innovation, and (3) 
through incremental growth from minimal structure, learning is
 significantly faster and stronger than with the best 
 fixed-topology methods. NEAT also shows that it is possible to evolve populations of increasingly large genomes, achieving 
 highly complex solutions that would otherwise be difficult to optimize.}, 
 booktitle = {Proceedings of the 4th Annual 
 Conference on Genetic and Evolutionary Computation}, 
 pages = {569–577}, 
 numpages = {9}, 
 location = {New York City, New York}, 
 series = {GECCO'02} 
}

%% Sobre el perceptrón 

@InProceedings{perceptron-convergence,
author="Van Der Malsburg, C.",
editor="Palm, G{\"u}nther
and Aertsen, Ad",
title="Frank Rosenblatt: Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
booktitle="Brain Theory",
year="1986",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="109-115",
abstract="Frank Rosenblatt's intention with his book, according to his own introduction, is not just to describe a machine, the perceptron, but rather to put forward a theory. He formulates a series of machines. Each machine serves to introduce a new concept.",
isbn="978-3-642-70911-1"
}
%% ------ Redes neuronales como aproximadores universales  -----
% Conferencia de 1987 sobre redes neuronales
@ARTICLE{4307059,
  author={},
  journal={IEEE Expert}, 
  title={IEEE First Annual International Conference on Neural Networks San Diego, California June 21-24, 1987}, 
  year={1987},
  volume={2},
  number={2},
  pages={14-14},
  doi={10.1109/MEX.1987.4307059}}

% Artículo principal: Multilayer feedforward networks are universal approximators
@article{HORNIK1989359,
title = {Multilayer feedforward networks are universal approximators},
journal = {Neural Networks},
volume = {2},
number = {5},
pages = {359-366},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}
% Imagen perceptrón multicapa  
@misc{alma991008058419704990,
publisher = {AMLbook.com},
title = {Learning from data : a short course },
year = {2012},
author = {Abu-Mostafa, Yaser S.},
address = {Seattle},
booktitle = {Learning from data : a short course},
isbn = {9781600490064},
keywords = {Aprendizaje automático},
language = {eng},
}
% Libro para algunas demostraciones de teoría de la medida
% Se cita en pag 77 para definición de función medible
% Se cita en pag 228 teorema 52.G
% Se cita en pag 241-242  teorema 55.C y 55.D
% Para el teorema de Lusin en pag 242-243.
@Book{ nla.cat-vn1819421,
author = { Halmos, Paul R. },
title = { Measure theory / [by] Paul R. Halmos },
isbn = { 0387900888 },
publisher = { Springer-Verlag New York },
year = { 1974 },
type = { Book },
url = { http://www.loc.gov/catdir/enhancements/fy0814/74010690-t.html },
language = { English },
subjects = { Measure theory. },
life-dates = { 1974 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn1819421 },
}

% Estimación del tamaño mini-batch
@article{mini-batch-size,
  author    = {Michael P. Perrone and
               Haidar Khan and
               Changhoan Kim and
               Anastasios Kyrillidis and
               Jerry Quinn and
               Valentina Salapura},
  title     = {Optimal Mini-Batch Size Selection for Fast Gradient Descent},
  journal   = {CoRR},
  volume    = {abs/1911.06459},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.06459},
  eprinttype = {arXiv},
  eprint    = {1911.06459},
  timestamp = {Mon, 02 Dec 2019 13:44:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-06459.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% ----- Backbones ----
% Artículo donde se enseña beneficios de utilizar backbones
@article{backbone-object-detection,
  author    = {Tingting Liang and
               Xiaojie Chu and
               Yudong Liu and
               Yongtao Wang and
               Zhi Tang and
               Wei Chu and
               Jingdong Chen and
               Haibin Ling},
  title     = {CBNetV2: {A} Composite Backbone Network Architecture for Object Detection},
  journal   = {CoRR},
  volume    = {abs/2107.00420},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.00420},
  eprinttype = {arXiv},
  eprint    = {2107.00420},
  timestamp = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-00420.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{backbone-Architecture,
  author    = {Shanghua Gao and
               Ming{-}Ming Cheng and
               Kai Zhao and
               Xinyu Zhang and
               Ming{-}Hsuan Yang and
               Philip H. S. Torr},
  title     = {Res2Net: {A} New Multi-scale Backbone Architecture},
  journal   = {CoRR},
  volume    = {abs/1904.01169},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.01169},
  eprinttype = {arXiv},
  eprint    = {1904.01169},
  timestamp = {Thu, 25 Apr 2019 10:24:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-01169.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Algoritmo genético para inicializar  pesos redes neuronales
@article{Montana2002NeuralNW,
  title={Neural Network Weight Selection Using Genetic Algorithms},
  author={David J. Montana},
  year={2002}
}

% Ejemplos de redes neuronales que combinan distintas funciones de activación 
% artículo que compara arquitecturas habituales 
@article{DBLP:journals/corr/abs-1811-03378,
  author    = {Chigozie Nwankpa and
               Winifred Ijomah and
               Anthony Gachagan and
               Stephen Marshall},
  title     = {Activation Functions: Comparison of trends in Practice and Research
               for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1811.03378},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.03378},
  eprinttype = {arXiv},
  eprint    = {1811.03378},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-03378.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% Ejemplo arquitectura 
@article{DBLP:journals/corr/SzegedyVISW15,
  author    = {Christian Szegedy and
               Vincent Vanhoucke and
               Sergey Ioffe and
               Jonathon Shlens and
               Zbigniew Wojna},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  journal   = {CoRR},
  volume    = {abs/1512.00567},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.00567},
  eprinttype = {arXiv},
  eprint    = {1512.00567},
  timestamp = {Mon, 13 Aug 2018 16:49:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Ejemplo paper general 
% Habla sobre cómo se combinan y funcionan las distintas funciones de activación 
@INPROCEEDINGS{8258768,
  author={Wang, Bin and Li, Tianrui and Huang, Yanyong and Luo, Huaishao and Guo, Dongming and Horng, Shi-Jinn},
  booktitle={2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)}, 
  title={Diverse activation functions in deep learning}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ISKE.2017.8258768}}

%%% ---- Sobre funciones de activación 
  % Artículo donde se introduce la función de cosine squasher
  @MISC{Gallant88thereexists,
    author = {A. Ronald Gallant and Halbert White},
    title = {There Exists A Neural Network That Does Not Make Avoidable Mistakes},
    year = {1988},
    abstract = {
    We show that a multiple input, single output, 
    single hidden layer feedforward network with (known) 
    hardwired connections from input to hidden layer, monotone squashing at the hidden layer
    and no squashing at the output embeds as a special case a "Fourier network" which yields a Fourier series
    approximation to a given function as its output. Thus, such networks possess all the approximation properties
    of Fourier series representations. In particular, approximation to any desired accuracy of any square 
    integrable function can be achieved by such a network, using sufficiently many hidden units. In this 
    sense, such networks do not make avoidable mistakes.
    }
}
% Generalización con funciones no continuas
@article{FUNAHASHI1989183,
title = {On the approximate realization of continuous mappings by neural networks},
journal = {Neural Networks},
volume = {2},
number = {3},
pages = {183-192},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90003-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900038},
author = {Ken-Ichi Funahashi},
keywords = {Neural network, Back propagation, Output function, Sigmoid function, Hidden layer, Unit, Realization, Continuous mapping},
abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.}
}
% Generalización del teorema universal con funciones de actvación no acotadas
@article{DBLP:journals/corr/SonodaM15,
  author    = {Sho Sonoda and
               Noboru Murata},
  title     = {Neural Network with Unbounded Activations is Universal Approximator},
  journal   = {CoRR},
  volume    = {abs/1505.03654},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.03654},
  eprinttype = {arXiv},
  eprint    = {1505.03654},
  timestamp = {Mon, 13 Aug 2018 16:47:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SonodaM15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%%         Funciones de activación  modernas
%% Hace y explica un recorrido sobre todas las funciones de activación

@article{modern-trainable-activation-functions,
  author    = {Andrea Apicella and
               Francesco Donnarumma and
               Francesco Isgr{\`{o}} and
               Roberto Prevete},
  title     = {A survey on modern trainable activation functions},
  journal   = {CoRR},
  volume    = {abs/2005.00817},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.00817},
  eprinttype = {arXiv},
  eprint    = {2005.00817},
  timestamp = {Thu, 04 Mar 2021 07:49:33 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-00817.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%            Condición para ser función activación
%%%%% ¡Basta con que sea no polinómica!
@ARTICLE{non-polynomial-activation-functions,
  author={Tianping Chen and Hong Chen},
  journal={IEEE Transactions on Neural Networks}, 
  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems}, 
  year={1995},
  volume={6},
  number={4},
  pages={911-917},
  doi={10.1109/72.392253}}

% Métodos de selección de funciones de activación
@article{FunctionOptimizationwithGeneticAlgorithms,
author = {Kolev, Kolyu and Sevova, Janeta and Blagoev, Ivan},
year = {2018},
month = {05},
pages = {},
title = {Artificial Neural Network Activation Function Optimization with Genetic Algorithms}
}
@INPROCEEDINGS{Genetic-deep-neural-networks,
  author={Zhang, Luna M.},
  booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
  title={Genetic deep neural networks using different activation functions for financial data mining}, 
  year={2015},
  volume={},
  number={},
  pages={2849-2851},
  doi={10.1109/BigData.2015.7364099}}


%%----------- Sobre utilizar otros cuerpos  --------------
% Utiliza solo aritmética entera 
% Lo que hace es adaptar backpropagation y forward propagation 
% con algoritmos de redondeo para así poder utilizar enteros (no tiene ningún sustengo teórico)
@ARTICLE{9709160,
  author={Wang, Maolin and Rasoulinezhad, Seyedramin and Leong, Philip H.W. and So, Hayden Kwok-Hay},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={NITI: Training Integer Neural Networks Using Integer-only Arithmetic}, 
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPDS.2022.3149787}
  }

%%% Sobre CPU y GPUS
@article{CPU-vs-GPUS,
author = {Lee, Victor and Kim, Changkyu and Chhugani, Jatin and Deisher, Michael and Kim, Daehyun and Nguyen, Anthony and Satish, Nadathur and Smelyanskiy, Mikhail and Chennupaty, Srinivas and Hammarlund, Per and Singhal, Ronak and Dubey, Pradeep},
year = {2010},
month = {01},
pages = {451-460},
title = {Debunking the 100x GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU},
volume = {38},
isbn = {978-1-4503-0053-7},
journal = {ACM SIGARCH Computer Architecture News},
doi = {10.1145/1816038.1816021}
}


%%%%----- Estado del arte   --------
%
% Este artículo """demuestra""" que para una serie de distribuciones de datos 
% si queremos aproximar "suavemente" es """necesario""" aumentar el número de neuronas
% en cierto factor "d" 
% Lo cierto es que se basa en la dimensión VC, conceptos que acotan en el peor de los casos 
@inproceedings{
a-universal-law-of-Robustness,
title={A Universal Law of Robustness via Isoperimetry},
author={Sebastien Bubeck and Mark Sellke},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=z71OSKqTFh7}
}

%
% Argumento en contra de aumentar el número de capas
@inproceedings{DBLP:conf/iwann/Linan-Villafranca21,
  author    = {Luis Li{\~{n}}{\'{a}}n{-}Villafranca and
               Mario Garc{\'{\i}}a{-}Valdez and
               Juan Juli{\'{a}}n Merelo and
               Pedro Castillo{-}Valdivieso},
  editor    = {Ignacio Rojas and
               Gonzalo Joya and
               Andreu Catal{\`{a}}},
  title     = {EvoMLP: {A} Framework for Evolving Multilayer Perceptrons},
  booktitle = {Advances in Computational Intelligence - 16th International Work-Conference
               on Artificial Neural Networks, {IWANN} 2021, Virtual Event, June 16-18,
               2021, Proceedings, Part {II}},
  series    = {Lecture Notes in Computer Science},
  volume    = {12862},
  pages     = {330--342},
  publisher = {Springer},
  year      = {2021},
  url       = {https://doi.org/10.1007/978-3-030-85099-9\_27},
  doi       = {10.1007/978-3-030-85099-9\_27},
  timestamp = {Thu, 26 Aug 2021 10:23:24 +0200},
  biburl    = {https://dblp.org/rec/conf/iwann/Linan-Villafranca21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Estado del arte de las redes neuronales profundas en visión por computador 
@article{CHAI2021100134,
title = {Deep learning in computer vision: A critical review of emerging techniques and application scenarios},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100134},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100134},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000670},
author = {Junyi Chai and Hao Zeng and Anming Li and Eric W.T. Ngai},
keywords = {Machine learning, Deep learning, Computer vision, Literature review},
abstract = {Deep learning has been overwhelmingly successful in computer vision (CV), natural language processing, and video/speech recognition. In this paper, our focus is on CV. We provide a critical review of recent achievements in terms of techniques and applications. We identify eight emerging techniques, investigate their origins and updates, and finally emphasize their applications in four key scenarios, including recognition, visual tracking, semantic segmentation, and image restoration. We recognize three development stages in the past decade and emphasize research trends for future works. The summarizations, knowledge accumulations, and creations could benefit researchers in the academia and participators in the CV industries.}
}

%%%%%%%%%%%% Alternativas para backpropagation  %%%%%%%%%%%%%%

% Algoritmo alternativo a backpropagation
% Pros: 
% - Menor coste computacional
% - Alternativa a backpropagation
% Cons: 
% - Resutados solo empíricos
% - Sigue necesistando hipótesis de derivabilidad 
% - Mantiene el coste en memoria constante

@article{forwardGradient,
  doi = {10.48550/ARXIV.2202.08587},
  
  url = {https://arxiv.org/abs/2202.08587},
  author = {Baydin, Atılım Güneş and Pearlmutter, Barak A. and Syme, Don and Wood, Frank and Torr, Philip},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.6; I.2.5, 68T07},
  title = {Gradients without Backpropagation},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{TransactionsOnNeuralNetworks,
  author={Wilamowski, Bogdan M. and Yu, Hao},
  journal={IEEE Transactions on Neural Networks}, 
  title={Neural Network Learning Without Backpropagation}, 
  year={2010},
  volume={21},
  number={11},
  pages={1793-1803},
  doi={10.1109/TNN.2010.2073482}}




  %% ---------- Nociones topología -------
 % De james1966topology tomamos: 
 % Definición vecino (pag 63)
 % Definición Hausdorff (pag 137-138)
 % Definción espacio Normal  Hausdorff (pag 144)
 % Caracterización de normalidad de Tietze (pag 149-151)
  @book{james1966topology,
  title={Topology},
  author={James Dugundji},
  isbn={9780205002719},
  lccn={66010940},
  series={Allyn and Bacon series in advanced mathematics},
  url={https://books.google.es/books?id=FgFRAAAAMAAJ},
  year={1966},
  publisher={Allyn and Bacon}
}

%% Teoría de distribución   
% Libro de referencia e introducción a la materia 
  @book{teoriaDistribuciones,
  title={Introduction to Fourier Analysis and Generalized Functions.},
  author={M. J. Lighthill},
  isbn={0-521-09128-4 },
  year={1958},
  publisher={Cambridge University Press},
  subjects = {Defines distributions as limits of sequences of functions under integrals}
}

%%%%%% Nocciones de estadística e inferencia estadística  %%%%%%%%%%%
% Conceptos básicos 
@book{OpenIntroStatistics,
  author         = {David Diez, Mine Çentinkaya-Runder, Christopher D Barr},
  publisher      = {Openinto},
  title          = {OpenIntro Statics},
  year           = {2022},
  url            = {https://github.com/OpenIntroStat/openintro-statistics}
}
% Enfoque más aplicado(el que vamos a usar en la práctica)
@book{BiologicalStatistics,
  author         = {McDonald, J.H.},
  publisher      = {Sparky House Publishing, Baltimore, Maryland},
  title          = {Handbook of Biological Statistics},
  year           = {2014}
}


@article{Liskov-principle,
 author = {Liskov, Barbara H. and Wing, Jeannette M.}, 
title = {A Behavioral Notion of Subtyping}, 
year = {1994}, 
issue_date = {Nov. 1994},
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA},
  volume = {16}, number = {6}, 
  issn = {0164-0925}, 
  url = {https://doi.org/10.1145/197320.197383}, 
  doi = {10.1145/197320.197383}, 
  abstract = {The use of hierarchy is an important component of object-oriented design.
   Hierarchy allows the use of type families, in which higher level supertypes capture the 
   behavior that all of their subtypes have in common. For this methodology to be 
   effective, it is necessary to have a clear understanding of how subtypes and supertypes 
   are related. This paper takes the position that the relationship should ensure that any 
   property proved about supertype objects also holds for its subtype objects. It presents 
   two ways of defining the subtype relation, each of which meets this criterion, and each 
   of which is easy for programmers to use. The subtype relation is based on the 
   specifications of the sub- and supertypes; the paper presents a way of specifying types 
   that makes it convenient to define the subtype relation. The paper also discusses the 
   ramifications of this notion of subtyping on the design of type families.}, 
   journal = {ACM Trans. Program. Lang. Syst.},
    month = {nov}, pages = {1811–1841}, 
    numpages = {31}, 
    keywords = {Larch, subtyping, formal specifications} }


%%%%%%%%%%%%%%%%%% metodología %%%%%%%%%%%%%%
%% Descripción del desarrollo ágil en la ciencia
@article{DBLP:journals/corr/abs-2104-12545,
  author    = {Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title     = {Agile (data) science: a (draft) manifesto},
  journal   = {CoRR},
  volume    = {abs/2104.12545},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.12545},
  eprinttype = {arXiv},
  eprint    = {2104.12545},
  timestamp = {Mon, 03 May 2021 17:38:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-12545.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% sobre cómo hacer un TFG
@online{que-es-un-trabajo-fin-de-x,
  author= {Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Cómo llevar a término un TFG/TFM en informática},
  url = {https://jj.github.io/que-es-un-trabajo-fin-de-x/tf.html},
  urldate = {2022-02-05}
}
%%--- PRÁCTICAS ÁGILES ---------------
% sobre metodología de personas 
% https://www.interaction-design.org/literature/article/personas-why-and-how-you-should-use-them
@online{personas-why-and-how-you-should-use-them,
  author= {Rikke Friis Dam and Teo Yu Siang |},
  title ={Personas – A Simple Introduction},
  url = {https://www.interaction-design.org/literature/article/personas-why-and-how-you-should-use-them},
  urldate = {2022-02-05}
}

% Cita historias de usuario 
@Book{UserStories,
author = {Mike Cohn},
title = {User Stories for Agile Software},
isbn = { 0-321-20568-5},
publisher = {Pearson Education, Inc.},
year = {2009},
type = { Book },
language = { English },
subjects = { Agile Software },
}


% - 12 principios manifiesto agil 
% https://agilemanifesto.org/iso/es/principles.html
@online{principios-manifiesto-agil,
  author= {	{Kent Beck,
Mike Beedle,
Arie van Bennekum,
Alistair Cockburn,
Ward Cunningham,
Martin Fowler,
James Grenning,
Jim Highsmith,
Andrew Hunt,
Ron Jeffries,
Jon Kern,
Brian Marick,
Robert C. Martin,
Steve Mellor,
Ken Schwaber,
Jeff Sutherland,
Dave Thomas}},
  title ={Principios del Manifiesto Ágil},
  url = {https://agilemanifesto.org/iso/es/principles.html},
  urldate = {2022-02-19}
}
% --- Por qué ágil no es una metodología 
@online{why-agile-is-not-a-methodology-1,
  author= {Charlie Rudd },
  title ={Why Agile is Not a Methodology},
  url = {https://www.solutionsiq.com/resource/blog-post/why-agile-is-not-a-methodology/},
  urldate = {2022-02-19}
}
@online{why-agile-is-not-a-methodology-2,
  author= {Geert Bossuyt},
  title ={Agile is not a methodology, it's a mindset !},
  url = {https://xebia.com/agile-is-not-a-methodology-its-a-mindset/},
  urldate = {2022-02-19}
}

% mi repositorio de github :D
% https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales
@online{TFG-Estudio-de-las-redes-neuronales,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s, Francisco Javier Mer{\'{i}} de la Maza },
  title ={Github, repositorio: Estudio de las redes neuronales},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales},
  urldate = {2022-02-05}
}
% HISTORIAS DE USUARIO 
% HU 01
@online{TFG-Estudio-de-las-redes-neuronales-HU01,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 1},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/48},
  urldate = {2022-02-13}
}
% HU 06
@online{TFG-Estudio-de-las-redes-neuronales-HU02,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 2},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/65},
  urldate = {2022-02-13}
}
% HU 03
@online{TFG-Estudio-de-las-redes-neuronales-HU03,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 3},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/50},
  urldate = {2022-02-13}
}
% HU 04
@online{TFG-Estudio-de-las-redes-neuronales-HU04,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 4},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/51},
  urldate = {2022-02-13}
}
% HU 05
@online{TFG-Estudio-de-las-redes-neuronales-HU05,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 5},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/64},
  urldate = {2022-02-13}
}
% HU 06
@online{TFG-Estudio-de-las-redes-neuronales-HU06,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s},
  title ={Github, repositorio: Estudio de las redes neuronales, Historia de usuario 6},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/issues/49},
  urldate = {2022-02-13}
}

%% Milestone
@online{TFG-Estudio-de-las-redes-neuronales-milestones,
  author= {Blanca Cano Camarero, Juan Juli{\'{a}}n Merelo Guerv{\'{o}}s, Francisco Javier Mer{\'{i}} de la Maza},
  title ={Github, repositorio: Estudio de las redes neuronales, Milestone},
  url = {https://github.com/BlancaCC/TFG-Estudio-de-las-redes-neuronales/milestones},
  urldate = {2022-02-13}
}

%%%%%% registro del número de horas de trabajo %%%%%%%%%%%%%
%% hoja de cálculo 
%% https://docs.google.com/spreadsheets/d/1TCcKQIKjKW9sMSU2f6obN9gHgv3c8UEdjmONkBlv42M/edit?usp=sharing
@online{TFG-hoja-calculo-horas-trabajo,
  author= {Blanca Cano Camarero},
  title ={Registro de trabajo en hoja de cálculo},
  url = {https://docs.google.com/spreadsheets/d/1TCcKQIKjKW9sMSU2f6obN9gHgv3c8UEdjmONkBlv42M/edit?usp=sharing},
  urldate = {2022-02-05}
}
