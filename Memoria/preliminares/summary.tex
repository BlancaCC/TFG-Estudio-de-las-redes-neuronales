% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter*{Summary}\label{ch:summary}
%\addcontentsline{toc}{chapter}{Summar}

Nowadays experimental research in Neural Networks is more advanced than theoretical
results. 
From this we aim to establish a solid mathematical theory so as to optimize the current neural network models. 


As a result of our study, we have proposed a novel neural 
network model, and adapted and optimized
evaluation and learning methods to it. 
Moreover, we have discovered some theorems that prove the 
equivalence among some activation functions, and hence we propose a new
 algorithm to initialize the weights of neural networks. Thanks to the
first result, we obtain a criterion to choose the most 
suitable activation function to maintain accuracy and reduce computational costs.
 Thanks to the second one, we might accelerate 
learning convergence methods.

In addition, the models, methods and algorithms have been 
implemented in Julia, resulting in the \textit{OptimizedNeuralNetwork.jl} library. 

All the theory development, designs, decisions and results are 
written in this memory, which has the following structure: 
\begin{itemize}
 \item \textbf{Chapter \ref{ch00:methodology}: Description of the methodology followed.} We have organized our project according to an agile philosophy  based on personas methodology, user stories, milestones and tests. This method has conducted and linked mathematical and technical results and implementations, giving them coherence and validation methods. 

 \item \textbf{Chapter \ref{chapter:Introduction-neuronal-networks}: Description of the learning problem.} We defined the characteristic and type of machine learning problems. We will focus on supervised learning ones. 

 \item \textbf{Chapter \ref{ch03:teoria-aproximar}:  Approximation theory.} In order to establish a solid theory, we will start our work trying to solve machine learning problems by traditional approximation methods.  The main result we prove is the Stone Weierstrass's theorem. As a conclusion of this chapter we will achieve knowledge of the virtues and faults of traditional methods and understanding the necessity of new methods and structures such as neural networks. 

 \item \textbf{Chapter \ref{chapter4:redes-neuronales-aproximador-universal}: Neural networks are universal approximators.}  In this chapter we introduce our neural 
 network model and compare it with the conventional ones. In order to show it is well 
 defined, we will prove the universal convergence of our model to any measurable 
 function. In addition, we will give some results about how our model solves 
 classification and regression problems as its number of neurons rises. Finally, we 
 will argue if all of those math results can actually solve real life problems. The 
 idea behind the debate is the computability representation of real numbers. 

 \item \textbf{Chapter  \ref{chapter:construir-redes-neuronales}: The design and implementation of neural networks.} We will carefully  describe the design and 
 implementation of our model of neural network. Thanks to that we will obtain some 
 mathematical results about bias and classification function. This will be useful to 
 compare our model with the conventional ones and justify 
our selection. Moreover, we will explain, justify and design  learning and evaluation 
methods to our model. These methods are optimized versions of Forward Propagation and 
Backpropagation. 

\item \textbf{Chapter \ref{funciones-activacion-democraticas-mas-demoscraticas}: Democratization of activation functions.} 
We will explain in this chapter if there are better activation functions. In this 
direction we will prove two original results which show that there are families of 
activation functions that under the same conditions will solve problems with the same 
accuracy. As a result, if we compare the computational cost of the members of those 
families and choose the faster one, we will obtain a method to optimize evaluation and 
learning of neural networks without loss of accuracy. We have used the Wilcoxon 
signed-rank test as a statistical hypothesis test so as to give a rigorous study of 
our criteria. 

\item \textbf{Chapter \ref{section:inicializar_pesos}: Weight initializing algorithm.} 
Since Backpropagation and other iterative  methods are sensitive to the initial 
value of a neural network, we will show an original method to initialize its weights 
from training data. This process not only will produce a better initial step but also 
has lower computational cost than Backpropagation.  To test the potential of this 
method we will use the Wilcoxon signed-rank test again and also, from the experiment's 
requirements we will design and create our OptimizedNeuralNetwork.jl library. In this chapter we 
will also explain every decision done during the design and implementation of the 
library in order to be as efficient as possible.	

\item \textbf{Chapter \ref{ch08:genetic-selection}: Use of genetic algorithm in the selection of activation function.} 
In this chapter we will explain a future work. Given a fixed number of neurons, the 
selection of its activation function may be crucial to reduce the train and test 
error.  However, adding more free params to the search space increases its complexity 
and at same time the cost of finding a solution.  Nevertheless, the result obtained in 
chapter \ref{funciones-activacion-democraticas-mas-demoscraticas} and a property of our neural model will reduce the space complexity.

\item \textbf{Chapter \ref{ch09:conclusion}: Conclusions.}
\end{itemize} 

\paragraph{KEYWORDS:}
\begin{itemize*}[label=,itemsep=1em,itemjoin=\hspace{1em}]
  \item neural networks
  \item optimization
  \item activation functions
  \item weights initializing
  \item machine learning library
\end{itemize*}

% Al finalizar el resumen en inglés, volvemos a seleccionar el idioma español para el documento
\selectlanguage{spanish}
\endinput
